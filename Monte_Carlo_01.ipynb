{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On #2 - Skating the Frozen Lake w/Monte Carlo\n",
    "\n",
    "## Goal:\n",
    "* Implement Monte Carlo\n",
    "* Explore different policies solving OpenAIGym : FrozenLake\n",
    "\n",
    "## Steps:\n",
    "1. Program Monte Carlo â€“ 1st Visit or every visit\n",
    "2. Run the code and understand how it all comes together\n",
    "3. How is the exploration policy implemented ?\n",
    "4. How do we finally get the learned policy from the agent  ?\n",
    "5. Does it correspond to our deterministic policy ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference : \n",
    "* Based on Udacity github https://github.com/udacity/deep-reinforcement-learning/tree/master/monte-carlo\n",
    "* plus my solution for the DQN https://github.com/xsankar/DQN_Navigation/blob/master/Navigation-v2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install the required packages\n",
    "\n",
    "* No esoteric requirements\n",
    "* You can run them without docker\n",
    "* pip install -r requirements.txt\n",
    "* Requirements\n",
    " * python 3.6, pytorch, openAI gym, numpy, matplotlib\n",
    " * anaconda is easier but not needed\n",
    " * Miniconda works fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Define imports\n",
    "\n",
    "python 3, numpy, matplotlib, torch, gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import gym\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Global Constants and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants Definitions\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "# Number of neurons in the layers of the Q Network\n",
    "FC1_UNITS = 16\n",
    "FC2_UNITS = 8\n",
    "FC3_UNITS = 4\n",
    "# Store models flag. Store during calibration runs and do not store during hyperparameter search\n",
    "STORE_MODELS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed : 0:00:10.001298\n"
     ]
    }
   ],
   "source": [
    "# Work area to quickly test utility functions\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "start_time = time.time()\n",
    "time.sleep(10)\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State :\n",
      " 0  1  2  3\n",
      " 4  5  6  7\n",
      " 8  9 10 11\n",
      "12 13 14 15\n",
      "\n",
      " S  F  F  F\n",
      " F  H  F  H\n",
      " F  F  F  H\n",
      " H  F  F  G\n"
     ]
    }
   ],
   "source": [
    "print(\"State :\")\n",
    "print(\" 0  1  2  3\")\n",
    "print(\" 4  5  6  7\")\n",
    "print(\" 8  9 10 11\")\n",
    "print(\"12 13 14 15\")\n",
    "print()\n",
    "print(\" S  F  F  F\")\n",
    "print(\" F  H  F  H\")\n",
    "print(\" F  F  F  H\")\n",
    "print(\" H  F  F  G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slippery Slope !\n",
    "#### The default FrozenLake-v0 is very slippery. Let us create an environment that is not slippery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78, # optimum = .8196\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Examine the State and Action Spaces\n",
    "\n",
    "The state space is 16, with four actions [ 0 = Left, 1 = Down, 2 = Right, 3 = Up ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n",
      "[0, 1, 2, 3]\n",
      "[ 0 = Left, 1 = Down, 2 = Right, 3 = Up ]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "act_space = [i for i in range(0,env.action_space.n)]\n",
    "print(act_space)\n",
    "# env.unwrapped.get_action_meanings() # AttributeError: 'FrozenLakeEnv' object has no attribute 'get_action_meanings'\n",
    "print('[ 0 = Left, 1 = Down, 2 = Right, 3 = Up ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States = 16\n",
      "Actions = 4\n",
      "{0: [(1.0, 0, 0.0, False)], 1: [(1.0, 4, 0.0, False)], 2: [(1.0, 1, 0.0, False)], 3: [(1.0, 0, 0.0, False)]}\n"
     ]
    }
   ],
   "source": [
    "#print(dir(env))\n",
    "#print(dir(env.unwrapped))\n",
    "print('States = {:d}'.format(env.unwrapped.nS))\n",
    "print('Actions = {:d}'.format(env.unwrapped.nA))\n",
    "print(env.unwrapped.P[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen Lake, but slippery no more !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test the environment with Random Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 ]  ->  0  = [ 0 ] R =  0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 0 ]  ->  3  = [ 0 ] R =  0.0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 0 ]  ->  1  = [ 4 ] R =  0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 4 ]  ->  0  = [ 4 ] R =  0.0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 4 ]  ->  3  = [ 0 ] R =  0.0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 0 ]  ->  3  = [ 0 ] R =  0.0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 0 ]  ->  3  = [ 0 ] R =  0.0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 0 ]  ->  3  = [ 0 ] R =  0.0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 0 ]  ->  1  = [ 4 ] R =  0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 4 ]  ->  3  = [ 0 ] R =  0.0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 0 ]  ->  1  = [ 4 ] R =  0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 4 ]  ->  2  = [ 5 ] R =  0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "End game! Reward:  0.0\n",
      "You lost :(\n",
      "\n",
      "[ 0 ]  ->  0  = [ 0 ] R =  0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 0 ]  ->  3  = [ 0 ] R =  0.0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 0 ]  ->  2  = [ 1 ] R =  0.0\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 1 ]  ->  0  = [ 0 ] R =  0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 0 ]  ->  0  = [ 0 ] R =  0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 0 ]  ->  0  = [ 0 ] R =  0.0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 0 ]  ->  2  = [ 1 ] R =  0.0\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 1 ]  ->  1  = [ 5 ] R =  0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "End game! Reward:  0.0\n",
      "You lost :(\n",
      "\n",
      "[ 0 ]  ->  2  = [ 1 ] R =  0.0\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 1 ]  ->  3  = [ 1 ] R =  0.0\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 1 ]  ->  3  = [ 1 ] R =  0.0\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 1 ]  ->  2  = [ 2 ] R =  0.0\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 2 ]  ->  0  = [ 1 ] R =  0.0\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 1 ]  ->  1  = [ 5 ] R =  0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "End game! Reward:  0.0\n",
      "You lost :(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(3):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        print('[',state,']',' -> ', action,' = [',next_state,']', 'R = ',reward)\n",
    "        env.render()\n",
    "        if done:\n",
    "            print('End game! Reward: ', reward)\n",
    "            print('You won :)\\n') if reward > 0 else print('You lost :(\\n')\n",
    "            break\n",
    "        else:\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Deterministic Optimal Policy\n",
    "aPolicy = {0:1,4:1,8:2,9:1,13:2,14:2}\n",
    "# [ 0 = Left, 1 = Down, 2 = Right, 3 = Up ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Frozen_Lake_Policy.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 ]  ->  1  = [ 4 ] 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 4 ]  ->  1  = [ 8 ] 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "[ 8 ]  ->  2  = [ 9 ] 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "[ 9 ]  ->  1  = [ 13 ] 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "[ 13 ]  ->  2  = [ 14 ] 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "[ 14 ]  ->  2  = [ 15 ] 1.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "End game! Reward:  1.0\n",
      "You won :)\n",
      "\n",
      "[ 0 ]  ->  1  = [ 4 ] 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 4 ]  ->  1  = [ 8 ] 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "[ 8 ]  ->  2  = [ 9 ] 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "[ 9 ]  ->  1  = [ 13 ] 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "[ 13 ]  ->  2  = [ 14 ] 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "[ 14 ]  ->  2  = [ 15 ] 1.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "End game! Reward:  1.0\n",
      "You won :)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(2): # Should be over in 6 steps, try for 2 episodes\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        policy_action = aPolicy.get(state,-1)\n",
    "        if policy_action == -1 :\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = policy_action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        print('[',state,']',' -> ', action,' = [',next_state,']', reward)\n",
    "        env.render()\n",
    "        if done:\n",
    "            print('End game! Reward: ', reward)\n",
    "            print('You won :)\\n') if reward > 0 else print('You lost :(\\n')\n",
    "            break\n",
    "        else:\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo\n",
    "### Let us implement the Basic MonteCarlo Algorithm\n",
    "<img src='Montecarlo_Alg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Define policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-Greedy\n",
    "\n",
    "<img src=\"e_greedy.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q,epsilon,nA):\n",
    "    policy = {}\n",
    "    for k in Q:\n",
    "        rand = np.random.random_sample()\n",
    "        if rand <= epsilon:\n",
    "            probs = [1.0/nA] * nA\n",
    "        else: # Exploit ie choose the highest action\n",
    "            probs = np.zeros(nA)\n",
    "            probs[np.argmax(Q[k])] = 1\n",
    "        policy[k] = probs\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = env.unwrapped.nS\n",
    "num_actions = env.unwrapped.nA\n",
    "epsilon_start = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_val = 0.3\n",
    "epsilon_alpha = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "def generate_episode_from_policy(env,policy):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        if state in policy:\n",
    "            probs = policy[state]\n",
    "        else:\n",
    "            probs = [1.0/num_actions] * num_actions\n",
    "        action = np.random.choice(np.arange(num_actions), p=probs)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # Reward shaping\n",
    "        if done:\n",
    "            if reward == 1: # Goal\n",
    "                reward = 10 \n",
    "            else: # Hole\n",
    "                reward = -10\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_basic(env, num_episodes, alpha=0.9):\n",
    "    # initialize empty dictionaries of arrays\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    epsilon = epsilon_start\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        ## decay epsilon as a factor of the episodes\n",
    "        epsilon = 1.0/(i_episode+0.5)\n",
    "        if (epsilon < epsilon_min):\n",
    "            epsilon = epsilon_min \n",
    "        #\n",
    "        # Or we could do an exponential decay\n",
    "        # epsilon = epsilon * epsilon_alpha\n",
    "        #\n",
    "        policy = epsilon_greedy(Q,epsilon,num_actions)\n",
    "        episode = generate_episode_from_policy(env,policy)\n",
    "        # Every-visit MC Prediction\n",
    "        g_t = 0\n",
    "        for i_visit in episode:\n",
    "            g_t += i_visit[2]\n",
    "        for i_visit in episode:\n",
    "            N[i_visit[0]][i_visit[1]] += 1.0\n",
    "            returns_sum[i_visit[0]][i_visit[1]] += g_t #i_visit[0][2]\n",
    "            Q[i_visit[0]][i_visit[1]] = returns_sum[i_visit[0]][i_visit[1]] / N[i_visit[0]][i_visit[1]]\n",
    "        # Replace the above for loop with the following one liner for constant-alpha MC control\n",
    "        #for i_visit in episode:\n",
    "        #   Q[i_visit[0]][i_visit[1]] = Q[i_visit[0]][i_visit[1]] + (alpha * (g_t - Q[i_visit[0]][i_visit[1]]))\n",
    "\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000/10000.Elapsed : 0:00:05.491907\n",
      "2019-02-28 18:48:52.627869\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "start_time = time.time()\n",
    "# obtain the estimated optimal policy and action-value function\n",
    "policy, Q = mc_basic(env, 10000)\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points to Ponder\n",
    "1. There are different ways of decaying $\\epsilon$\n",
    "2. Or we could just keep $\\epsilon$=1, making it uniformly random; but this won't work\n",
    " * We will see some interesting results\n",
    "3. GLIE â€“ decay Îµ as decreasing function of episodes\n",
    "4. Exponentially decay Îµ \n",
    " * The $\\epsilon$ decay is to incorporate the learning and make the action more deterministic\n",
    "5. We could use softmax and keep the policy constant ie weighted random.\n",
    " * As it learns, it's belief of the grid world changes and that shows up in the preferences\n",
    "5. After 1,000,000 episodes, it usually wins all the time.\n",
    " * I had the $\\epsilon$-min = 0.1. Probably it should be reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 = [0. 1. 0. 0.]\n",
      "1 = [1. 0. 0. 0.]\n",
      "2 = [1. 0. 0. 0.]\n",
      "3 = [1. 0. 0. 0.]\n",
      "4 = [0. 1. 0. 0.]\n",
      "6 = [0. 0. 0. 1.]\n",
      "8 = [0. 0. 1. 0.]\n",
      "9 = [0. 0. 1. 0.]\n",
      "10 = [0. 1. 0. 0.]\n",
      "13 = [0. 0. 1. 0.]\n",
      "14 = [0. 0. 1. 0.]\n",
      "0 - [-9.73252987  5.94728498  1.54867257  2.9707113 ]\n",
      "1 - [  1.57689812 -10.          -7.73584906  -8.47222222]\n",
      "2 - [-2.68882175 -4.58823529 -3.79310345 -6.35036496]\n",
      "3 - [ -3.57142857 -10.         -10.         -10.        ]\n",
      "4 - [ -9.1367173    6.74067868 -10.          -0.6581741 ]\n",
      "6 - [-10.          -4.54545455 -10.          -1.67192429]\n",
      "8 - [ -4.66155811 -10.           7.82788258  -2.28971963]\n",
      "9 - [  0.92783505   6.1516035    8.23452846 -10.        ]\n",
      "10 - [ -1.75792507   9.91490396 -10.          -2.48322148]\n",
      "13 - [-10.          -7.60683761   8.90944499  -6.61971831]\n",
      "14 - [ 8.1025641   7.44270205 10.          9.57559682]\n"
     ]
    }
   ],
   "source": [
    "# print(policy)\n",
    "for k, v in sorted(policy.items()):\n",
    "    print(k,\"=\",v)\n",
    "for k, v in sorted(Q.items()):\n",
    "    print(k,'-',v)\n",
    "#[ 0 = Left, 1 = Down, 2 = Right, 3 = Up ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Frozen_Lake_Policy.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 ]  ->  1  = [ 4 ] 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 4 ]  ->  1  = [ 8 ] 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "[ 8 ]  ->  2  = [ 9 ] 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "[ 9 ]  ->  2  = [ 10 ] 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "[ 10 ]  ->  1  = [ 14 ] 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "[ 14 ]  ->  2  = [ 15 ] 1.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "End game! Reward:  1.0\n",
      "You won :)\n",
      "\n",
      "[ 0 ]  ->  1  = [ 4 ] 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[ 4 ]  ->  1  = [ 8 ] 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "[ 8 ]  ->  2  = [ 9 ] 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "[ 9 ]  ->  2  = [ 10 ] 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "[ 10 ]  ->  1  = [ 14 ] 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "[ 14 ]  ->  2  = [ 15 ] 1.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "End game! Reward:  1.0\n",
      "You won :)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(2): # Should be over in 6 steps, try for 2 episodes\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        if state in policy:\n",
    "            probs = policy[state]\n",
    "        else:\n",
    "            probs = [1.0/num_actions] * num_actions\n",
    "        action = np.random.choice(np.arange(num_actions), p=probs)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        print('[',state,']',' -> ', action,' = [',next_state,']', reward)\n",
    "        env.render()\n",
    "        if done:\n",
    "            print('End game! Reward: ', reward)\n",
    "            print('You won :)\\n') if reward > 0 else print('You lost :(\\n')\n",
    "            break\n",
    "        else:\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _That's All, Folks !_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
