{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On #11 : Balancing the Cart Pole w/ DDPG!\n",
    "---\n",
    "\n",
    "### Goal:\n",
    "- Implement DDPG on the CartPole Environment\n",
    "    * It is an overkill, but we will get a good understanding and also can compare against other algorithms\n",
    "    \n",
    "### Steps:\n",
    "1. Program DDPG Algorithm\n",
    "2. Run and Optimize\n",
    "3. Plot Values, like we did in other exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Organization\n",
    "#### The program has 3 parts :\n",
    "- Part 1 Defines the classes, initiates the environment and so forth. It sets up all the scaffolding needed\n",
    "- Part 2 Explore and Learn - it performs the DDPG Reinforcement Learning. It also saves the best model\n",
    "- Part 3 Run saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Definitions & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Install the required packages\n",
    "\n",
    "The required setup is detailed in the README.md\n",
    "\n",
    "I am running this on a MacBookPro 14,3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define imports\n",
    "\n",
    "python 3, numpy, matplotlib, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import copy\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants Definitions\n",
    "BUFFER_SIZE = 4096 # 2048 # 512 # int(1e5) # int(1e6) # int(1e5)  # replay buffer size  ?\n",
    "BATCH_SIZE = 64 # 32 # 128 # 64 # 256        # minibatch size for training\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 0.05 # 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 0.01 # 5e-4 # 1e-4 # 0.001 # 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 0.001 # 6e-4 # 3e-4  # 3e-3 # 0.001 # 3e-4        # learning rate of the critic 0.001\n",
    "WEIGHT_DECAY = 0.0001   # L2 weight decay\n",
    "# Number of neurons in the layers of the Actor & Critic Networks\n",
    "FC_UNITS_ACTOR = [16,8] # [4,4] # [32,16] #[400,300] #[8,8] #[128,128] # [64,128] # [32,16] # [400,300] # [128,128]\n",
    "FC_UNITS_CRITIC = [16,8] #[4,4] # [32,16] #[400,300] #[8,8] #[128,128] # [64,128] # [32,16] # [400,300] # [128,128]\n",
    "# Store models flag. Store during calibration runs and do not store during hyperparameter search\n",
    "# Used in Part 3 to run a stored model\n",
    "STORE_MODELS = False # True - Turn it on when you are ready to do the calibration training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import gym, PIL\\nenv = gym.make('CartPole-v0')\\narray = env.reset()\\nPIL.Image.fromarray(env.render(mode='rgb_array'))\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import gym, PIL\n",
    "env = gym.make('CartPole-v0')\n",
    "array = env.reset()\n",
    "PIL.Image.fromarray(env.render(mode='rgb_array'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Create instance & Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01258566, -0.00156614,  0.04207708, -0.00180545])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(42)\n",
    "# array = env.reset()\n",
    "env.reset()\n",
    "# env.render()\n",
    "#PIL.Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Examine the State and Action Spaces\n",
    "\n",
    "* The state space is continuous, with an observation space of 4 \n",
    "    * {x,$\\dot{x}$,$\\theta$, theta_dot}\n",
    "        * Cart Position,  Cart Velocity, Pole Angle, Pole Velocity at tip\n",
    "        * The angle, probably, is in radians\n",
    "\n",
    "The action space, on the contrary is simple viz. 0 = Left, 1 = Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n",
      "[0, 1]\n",
      "[ 0 = Left, 1 = Right ]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "act_space = [i for i in range(0,env.action_space.n)]\n",
    "print(act_space)\n",
    "# env.unwrapped.get_action_meanings() # AttributeError: 'FrozenLakeEnv' object has no attribute 'get_action_meanings'\n",
    "print('[ 0 = Left, 1 = Right ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_elapsed_seconds', '_elapsed_steps', '_episode_started_at', '_max_episode_seconds', '_max_episode_steps', '_past_limit', 'action_space', 'class_name', 'close', 'compute_reward', 'env', 'metadata', 'observation_space', 'render', 'reset', 'reward_range', 'seed', 'spec', 'step', 'unwrapped']\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'action_space', 'close', 'force_mag', 'gravity', 'kinematics_integrator', 'length', 'masscart', 'masspole', 'metadata', 'np_random', 'observation_space', 'polemass_length', 'render', 'reset', 'reward_range', 'seed', 'spec', 'state', 'step', 'steps_beyond_done', 'tau', 'theta_threshold_radians', 'total_mass', 'unwrapped', 'viewer', 'x_threshold']\n",
      "States =  Box(4,)\n",
      "Actions =  Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(dir(env))\n",
    "print(dir(env.unwrapped))\n",
    "# To see what functions and variables are availabe\n",
    "print('States = ',env.unwrapped.observation_space)\n",
    "print('Actions = ',env.unwrapped.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test the environment with Random Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [ 0.00560942  0.01842265 -0.03590751 -0.0120678 ] ] -> 0  : [ [ 0.00597787 -0.17616644 -0.03614886  0.26907314] ] R= 1.0\n",
      "[ [ 0.00597787 -0.17616644 -0.03614886  0.26907314] ] -> 1  : [ [ 0.00245454  0.01945224 -0.0307674  -0.03478864] ] R= 1.0\n",
      "[ [ 0.00245454  0.01945224 -0.0307674  -0.03478864] ] -> 1  : [ [ 0.00284359  0.21500159 -0.03146317 -0.33701802] ] R= 1.0\n",
      "[ [ 0.00284359  0.21500159 -0.03146317 -0.33701802] ] -> 0  : [ [ 0.00714362  0.02034118 -0.03820353 -0.05442056] ] R= 1.0\n",
      "[ [ 0.00714362  0.02034118 -0.03820353 -0.05442056] ] -> 1  : [ [ 0.00755044  0.2159895  -0.03929194 -0.35890801] ] R= 1.0\n",
      "[ [ 0.00755044  0.2159895  -0.03929194 -0.35890801] ] -> 1  : [ [ 0.01187023  0.41164734 -0.0464701  -0.66371712] ] R= 1.0\n",
      "[ [ 0.01187023  0.41164734 -0.0464701  -0.66371712] ] -> 1  : [ [ 0.02010318  0.60738391 -0.05974445 -0.97066231] ] R= 1.0\n",
      "[ [ 0.02010318  0.60738391 -0.05974445 -0.97066231] ] -> 1  : [ [ 0.03225086  0.80325467 -0.07915769 -1.28149866] ] R= 1.0\n",
      "[ [ 0.03225086  0.80325467 -0.07915769 -1.28149866] ] -> 1  : [ [ 0.04831595  0.99929074 -0.10478767 -1.59788005] ] R= 1.0\n",
      "[ [ 0.04831595  0.99929074 -0.10478767 -1.59788005] ] -> 1  : [ [ 0.06830176  1.19548692 -0.13674527 -1.92131128] ] R= 1.0\n",
      "[ [ 0.06830176  1.19548692 -0.13674527 -1.92131128] ] -> 1  : [ [ 0.0922115   1.39178778 -0.17517149 -2.25309178] ] R= 1.0\n",
      "[ [ 0.0922115   1.39178778 -0.17517149 -2.25309178] ] -> 0  : [ [ 0.12004726  1.19869297 -0.22023333 -2.0191195 ] ] R= 1.0\n",
      "Episode 1 finished after 12 steps with a Total Reward = 12\n",
      "[ [-0.03056663  0.02063487  0.01650017  0.04869769] ] -> 0  : [ [-0.03015393 -0.17471974  0.01747412  0.34654056] ] R= 1.0\n",
      "[ [-0.03015393 -0.17471974  0.01747412  0.34654056] ] -> 1  : [ [-0.03364832  0.02014935  0.02440493  0.05941868] ] R= 1.0\n",
      "[ [-0.03364832  0.02014935  0.02440493  0.05941868] ] -> 0  : [ [-0.03324534 -0.17531386  0.02559331  0.35970053] ] R= 1.0\n",
      "[ [-0.03324534 -0.17531386  0.02559331  0.35970053] ] -> 0  : [ [-0.03675161 -0.3707901   0.03278732  0.66034247] ] R= 1.0\n",
      "[ [-0.03675161 -0.3707901   0.03278732  0.66034247] ] -> 0  : [ [-0.04416742 -0.56635261  0.04599417  0.96316632] ] R= 1.0\n",
      "[ [-0.04416742 -0.56635261  0.04599417  0.96316632] ] -> 0  : [ [-0.05549447 -0.76206137  0.06525749  1.26993652] ] R= 1.0\n",
      "[ [-0.05549447 -0.76206137  0.06525749  1.26993652] ] -> 0  : [ [-0.0707357  -0.957953    0.09065622  1.58232061] ] R= 1.0\n",
      "[ [-0.0707357  -0.957953    0.09065622  1.58232061] ] -> 1  : [ [-0.08989476 -0.76401924  0.12230263  1.31923099] ] R= 1.0\n",
      "[ [-0.08989476 -0.76401924  0.12230263  1.31923099] ] -> 0  : [ [-0.10517514 -0.96045669  0.14868725  1.64755357] ] R= 1.0\n",
      "[ [-0.10517514 -0.96045669  0.14868725  1.64755357] ] -> 1  : [ [-0.12438427 -0.76735361  0.18163833  1.40464805] ] R= 1.0\n",
      "[ [-0.12438427 -0.76735361  0.18163833  1.40464805] ] -> 1  : [ [-0.13973135 -0.57489151  0.20973129  1.17381268] ] R= 1.0\n",
      "Episode 2 finished after 11 steps with a Total Reward = 11\n",
      "[ [-0.0313163   0.03775876 -0.01135978 -0.01095196] ] -> 0  : [ [-0.03056113 -0.15719845 -0.01157882  0.27812529] ] R= 1.0\n",
      "[ [-0.03056113 -0.15719845 -0.01157882  0.27812529] ] -> 0  : [ [-0.0337051  -0.35215332 -0.00601632  0.56713389] ] R= 1.0\n",
      "[ [-0.0337051  -0.35215332 -0.00601632  0.56713389] ] -> 1  : [ [-0.04074816 -0.15694749  0.00532636  0.27256166] ] R= 1.0\n",
      "[ [-0.04074816 -0.15694749  0.00532636  0.27256166] ] -> 1  : [ [-0.04388711  0.03809805  0.01077759 -0.01843656] ] R= 1.0\n",
      "[ [-0.04388711  0.03809805  0.01077759 -0.01843656] ] -> 1  : [ [-0.04312515  0.2330638   0.01040886 -0.30769964] ] R= 1.0\n",
      "[ [-0.04312515  0.2330638   0.01040886 -0.30769964] ] -> 1  : [ [-0.03846388  0.4280359   0.00425487 -0.5970818 ] ] R= 1.0\n",
      "[ [-0.03846388  0.4280359   0.00425487 -0.5970818 ] ] -> 0  : [ [-0.02990316  0.23285466 -0.00768677 -0.30306167] ] R= 1.0\n",
      "[ [-0.02990316  0.23285466 -0.00768677 -0.30306167] ] -> 1  : [ [-0.02524607  0.42808532 -0.013748   -0.59815889] ] R= 1.0\n",
      "[ [-0.02524607  0.42808532 -0.013748   -0.59815889] ] -> 0  : [ [-0.01668436  0.2331584  -0.02571118 -0.30983793] ] R= 1.0\n",
      "[ [-0.01668436  0.2331584  -0.02571118 -0.30983793] ] -> 1  : [ [-0.01202119  0.42863707 -0.03190794 -0.61051727] ] R= 1.0\n",
      "[ [-0.01202119  0.42863707 -0.03190794 -0.61051727] ] -> 0  : [ [-0.00344845  0.23397531 -0.04411828 -0.3280526 ] ] R= 1.0\n",
      "[ [-0.00344845  0.23397531 -0.04411828 -0.3280526 ] ] -> 1  : [ [ 0.00123106  0.42969667 -0.05067933 -0.63431554] ] R= 1.0\n",
      "[ [ 0.00123106  0.42969667 -0.05067933 -0.63431554] ] -> 1  : [ [ 0.00982499  0.62548754 -0.06336564 -0.94251813] ] R= 1.0\n",
      "[ [ 0.00982499  0.62548754 -0.06336564 -0.94251813] ] -> 0  : [ [ 0.02233474  0.43127402 -0.08221601 -0.67039954] ] R= 1.0\n",
      "[ [ 0.02233474  0.43127402 -0.08221601 -0.67039954] ] -> 1  : [ [ 0.03096022  0.62743695 -0.095624   -0.98779431] ] R= 1.0\n",
      "[ [ 0.03096022  0.62743695 -0.095624   -0.98779431] ] -> 1  : [ [ 0.04350896  0.82370014 -0.11537988 -1.30891479] ] R= 1.0\n",
      "[ [ 0.04350896  0.82370014 -0.11537988 -1.30891479] ] -> 0  : [ [ 0.05998296  0.63021347 -0.14155818 -1.05446096] ] R= 1.0\n",
      "[ [ 0.05998296  0.63021347 -0.14155818 -1.05446096] ] -> 0  : [ [ 0.07258723  0.4372227  -0.1626474  -0.80934968] ] R= 1.0\n",
      "[ [ 0.07258723  0.4372227  -0.1626474  -0.80934968] ] -> 1  : [ [ 0.08133169  0.634155   -0.17883439 -1.14845725] ] R= 1.0\n",
      "[ [ 0.08133169  0.634155   -0.17883439 -1.14845725] ] -> 0  : [ [ 0.09401479  0.44176002 -0.20180354 -0.91676484] ] R= 1.0\n",
      "[ [ 0.09401479  0.44176002 -0.20180354 -0.91676484] ] -> 1  : [ [ 0.10284999  0.63895416 -0.22013883 -1.26548183] ] R= 1.0\n",
      "Episode 3 finished after 21 steps with a Total Reward = 21\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(3):\n",
    "    state = env.reset()\n",
    "    tot_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        print('[',state,']','->', action,' : [',next_state,']', 'R=',reward)\n",
    "        # env.render()\n",
    "        tot_reward += reward\n",
    "        steps += 1\n",
    "        if done:\n",
    "            print('Episode {:d} finished after {:d} steps with a Total Reward = {:.0f}'.\n",
    "                  format(i_episode+1,steps, tot_reward))\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "# Pole angle +/-12 degrees, Cart Pos +/- 2.4 or 200 steps\n",
    "# Cart Pos, Velocity, Pole Angle, Velocity\n",
    "# 12 degrees = .2094 radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device = {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Algorithm\n",
    "\n",
    "We are using the DDPG. I liked the simple systems diagram that Prof. Sergey Levine from UC Berkeley uses for his CS294 Deep Reinforcement Learning class[http://rail.eecs.berkeley.edu/deeprlcourse/]\n",
    "<img src=\"RL_Systems_Flow.png\">\n",
    "\n",
    "The major components of the algorithm are:\n",
    "1. `Actor` implemented as a Deep Neural Network whih consists of fully connected layers.\n",
    "2. `Critic` implemented as a Deep Neural Network which consists of fully connected networks\n",
    "3. `Experience replay buffer` - in order to train the network we take actions and then store the results in the replay buffer. The replay buffer is a circular buffer and it has methods to sample a random batch\n",
    "3. `The Agent` brings all of the above together. It interacts with the environment by taking actions based on a policy, collects rewards and the observation feedback, then stores the experience in the replay buffer and also initiates a learning step on the actor and critic networks\n",
    "     * The agent has 3 main components viz:\n",
    "         1. The DDPG Orchestrator which interacts with the environment by taking actions and then unpacking the returned package to rewards, state space et al.\n",
    "         2. It also has to do housekeeping like tracking scores, store high performant models and check when the problem is solved\n",
    "         3. The 3rd component is the most interesting one - it gives the agent the capability to hunt for the right policy !!\n",
    "\n",
    "### Pseudo Code\n",
    "<img src=\"DDPG_Alg.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size=state_size, action_size=action_size, seed=42, fc_units=FC_UNITS_ACTOR):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state, defaults to the global state size from the env\n",
    "            action_size (int): Dimension of each action, defaults to the global action size from the env\n",
    "            seed (int): Random seed\n",
    "            fc_units (list(int)): Number of nodes in the hidden layers as a list\n",
    "            ** Hard coded as a 3 layer network \n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            #nn.BatchNorm1d(state_size),\n",
    "            nn.Linear(state_size,fc_units[0]),\n",
    "            nn.ReLU(),\n",
    "            #nn.BatchNorm1d(fc_units[0]),\n",
    "            nn.Linear(fc_units[0],fc_units[1]),\n",
    "            nn.ReLU(),\n",
    "            #nn.BatchNorm1d(fc_units[1]),\n",
    "            nn.Linear(fc_units[1],action_size),\n",
    "            # nn.Tanh() # for continuous -1 to +1\n",
    "            nn.Softmax(dim=-1)\n",
    "            # nn.Sigmoid() # for 0-1\n",
    "        )\n",
    "        self.model.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self,m):\n",
    "        if (type(m) == nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            #nn.init.xavier_normal_(m.weight)\n",
    "            # nn.init.kaiming_normal_(m.weight)\n",
    "            m.bias.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        return self.model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size = state_size, action_size = action_size, seed=42, fc_units=FC_UNITS_CRITIC):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state, defaults to the global state size from the env\n",
    "            action_size (int): Dimension of each action, defaults to the global action size from the env\n",
    "            seed (int): Random seed\n",
    "            fc_units (list(int)): Number of nodes in the hidden layers as a list\n",
    "            ** Hard coded as a 3 layer network \n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.hc_1 = nn.Sequential(\n",
    "            nn.Linear(state_size,fc_units[0]),\n",
    "            nn.ReLU(), # leaky relu ?\n",
    "            # nn.BatchNorm1d(fc_units[0])\n",
    "        )\n",
    "        self.hc_2 = nn.Sequential(\n",
    "            nn.Linear(fc_units[0]+action_size,fc_units[1]),\n",
    "            nn.ReLU(), # leaky relu ?\n",
    "            nn.Linear(fc_units[1],1)\n",
    "        )\n",
    "        # Initialize the layers\n",
    "        self.hc_1.apply(self.init_weights)\n",
    "        self.hc_2.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = self.hc_1(state)\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = self.hc_2(x)\n",
    "        return (x)\n",
    "    \n",
    "    def init_weights(self,layer):\n",
    "        if (type(layer) == nn.Linear):\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            layer.bias.data.fill_(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed=42):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=False): #True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        #print(state)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return action # np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6. Instantiate an agent\n",
    "\n",
    "The state space and the action space dimensions come from the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=2, bias=True)\n",
      "    (5): Softmax()\n",
      "  )\n",
      ")\n",
      "Critic(\n",
      "  (hc_1): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hc_2): Sequential(\n",
      "    (0): Linear(in_features=18, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=42)\n",
    "print(agent.actor_local)\n",
    "print(agent.critic_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Learn & Train\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. DDPG Algorithm\n",
    "\n",
    "Define the DDPG Algorithm. Once we have defined the foundations (network, buffer, actor, critic, agent and so forth), the DDPG is relatively easy. It has a few responsibilities:\n",
    "1. Orchastrate the episodes calling the appropriate methods\n",
    "2. Display a running commentry of the scores and episode count\n",
    "3. Check the success criterion for solving the environment i.e. if running average is > 30 and print the episode count\n",
    "4. Store the model with the maximum score\n",
    "5. Keep track of the scores for analytics at the end of the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=1000):\n",
    "    scores_window = deque(maxlen=100)\n",
    "    scores = []\n",
    "    score  = 0\n",
    "    max_score = -np.Inf\n",
    "    has_seen_30 = False\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()                    # reset the environment    \n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        max_steps = 0\n",
    "        while True:\n",
    "            action = agent.act(state) # treat as softmax probabilities\n",
    "            act = int(np.random.choice(action_size, p=action)) # for Softmax\n",
    "            # act = 1 if action > 0.5 else 0 # for Sigmoid\n",
    "            next_state, reward, done, _ = env.step(act)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "            max_steps += 1\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:6.2f}\\tScore: {:6.2f}\\tMax_steps : {:3d}'.\\\n",
    "              format(i_episode, np.mean(scores_window), score, max_steps), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))  \n",
    "        if (np.mean(scores_window) >= 195.0) and (not has_seen_30):\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:5.2f}'.\\\n",
    "                  format(i_episode-100, np.mean(scores_window)))\n",
    "            # torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            has_seen_30 = True\n",
    "            break # Early stop\n",
    "            # To see how far it can go comment the break out\n",
    "        # Store the best model if desired\n",
    "        if STORE_MODELS:\n",
    "            if np.mean(scores_window) > max_score:\n",
    "                max_score = np.mean(scores_window)\n",
    "                torch.save(agent, 'checkpoint.pth')\n",
    "                # print(' .. Storing with score {}'.format(max_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. The actual training Run\n",
    "\n",
    "1. Run the DDPG\n",
    "2. Calculate and display end-of-run analytics viz. descriptive statistics and a plot of the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 9.8282\tScore:   8.00\tMax_steps :   7\n",
      "Episode 200\tAverage Score: 11.033\tScore:  14.00\tMax_steps :  13\n",
      "Episode 300\tAverage Score: 11.300\tScore:   8.00\tMax_steps :   7\n",
      "Episode 400\tAverage Score: 13.588\tScore:  11.00\tMax_steps :  10\n",
      "Episode 500\tAverage Score: 16.977\tScore:  11.00\tMax_steps :  10\n",
      "Episode 600\tAverage Score: 22.666\tScore:  26.00\tMax_steps :  25\n",
      "Episode 700\tAverage Score: 20.444\tScore:  12.00\tMax_steps :  11\n",
      "Episode 800\tAverage Score: 89.944\tScore: 200.00\tMax_steps : 199\n",
      "Episode 875\tAverage Score: 195.33\tScore: 200.00\tMax_steps : 199\n",
      "Environment solved in 775 episodes!\tAverage Score: 195.33\n",
      "Elapsed : 0:02:12.733915\n",
      "2019-01-18 09:19:16.580979\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=2, bias=True)\n",
      "    (5): Softmax()\n",
      "  )\n",
      ")\n",
      "Max Score 200.00 at 760\n",
      "Percentile [25,50,75] : [10. 14. 24.]\n",
      "Variance : 3747.562\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "scores = ddpg(n_episodes=1000) # 2000,1500 ; quick test 100,500\n",
    "env.close() # Close the environment\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())\n",
    "#\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "print(agent.actor_local)\n",
    "# print(agent.critic_local)\n",
    "print('Max Score {:.2f} at {}'.format(np.max(scores), np.argmax(scores)))\n",
    "print('Percentile [25,50,75] : {}'.format(np.percentile(scores,[25,50,75])))\n",
    "print('Variance : {:.3f}'.format(np.var(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Run\n",
    "<img src=\"ddpg_run.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Run logs & Notes\n",
    "\n",
    "1. A place to keep the statistics and qualitative observations\n",
    "2. It is easier to keep notes as soon as a run is done\n",
    "3. Also a place to keep future explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs\n",
    "#### 1/13/19\n",
    "\n",
    "1. Tried Softmax for actor, tried Sigmoid (with action_size=1) - was tanh()\n",
    "    * LR : Actor = 0.01, Critic = 0.001\n",
    "    * network 8 X 8\n",
    "1. Doesn't go beyond 10-11 steps when episode = 1500, buffer = 512\n",
    "2. No clue when it will wake up - episode = 15000; no dice so far\n",
    "3. Buffer = 2048 no change\n",
    "4. Took out batchnorm\n",
    "5. Back to softmax\n",
    "6. Took out clip(-1,1)\n",
    "    * No change score ~10\n",
    "7. tau = 0.05 - started learning - Am seeing the scores in the 20s! See if it passes nope stays at 20s\n",
    "8. network 400 X 300 ! goes back to a score of 10\n",
    "1. Network 36 X 8 ! Nope no good\n",
    "1. Network 4 X 4 - very small !\n",
    "1. Buffer = 4096, Batch = 32 meanders around a score of 10 for 10000 episodes !\n",
    "1. softmax(-1) definite progress - goes upto 200, but then gets 10 as well ! After 1100, it is steady !\n",
    " * Solved in 1029 episodes !\n",
    " * Was about to throw the towel ! Took me a day ! Was going to go back to A2C !\n",
    " * From 1300 onwards it gets prfect 200\n",
    " * __added noise and that saved the day !__\n",
    "1. back to 16 X 8 Network, batch = 64, 1500 episodes (was 15,000 ! and didn't do any good. The progress of a learning network is evident)\n",
    " * would have solved under 300, but a few black sheep epidodes bring the average down ;o(\n",
    " * 860 episodes. Between 300 and 800, it went down and came back !\n",
    "1. Cleaned up a little bit and ran again - 282 Episodes !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Run a stored Model or the learned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Here we are saving and loading the whole model. We cal also save & load the state dict\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from the saved file\n",
    "# The file has the parameters of the model that has the highest score during training\n",
    "# agent = torch.load('checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :  1 Score : 200.00 Steps : 199\n",
      "Episode :  2 Score : 200.00 Steps : 199\n",
      "Episode :  3 Score : 200.00 Steps : 199\n",
      "Episode :  4 Score : 200.00 Steps : 199\n",
      "Episode :  5 Score : 200.00 Steps : 199\n",
      "Episode :  6 Score : 200.00 Steps : 199\n",
      "Episode :  7 Score : 200.00 Steps : 199\n",
      "Episode :  8 Score : 200.00 Steps : 199\n",
      "Episode :  9 Score : 200.00 Steps : 199\n",
      "Episode : 10 Score : 200.00 Steps : 199\n",
      "Mean of 10 episodes = 200.0\n",
      "Elapsed : 0:00:00.449377\n",
      "2019-01-18 09:22:41.946488\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "scores=[]\n",
    "for i in range(10): # 10 episodes\n",
    "    state = env.reset()                                # reset the environment & get the current state\n",
    "    score = 0                                          # initialize the score\n",
    "    steps = 0                                          # Keep track of the number of steps\n",
    "    while True:\n",
    "        action = agent.act(state)                      # select an action, treat as softmax probabilities\n",
    "        act = int(np.random.choice(action_size, p=action)) # for Softmax\n",
    "        next_state, reward, done, _ = env.step(act)    # send the action to the environment\n",
    "        score += reward                                # update the score\n",
    "        state = next_state                             # roll over the state to next time step\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            break\n",
    "        else:\n",
    "            steps += 1\n",
    "    scores.append(score)\n",
    "    print(\"Episode : {:2d} Score : {:5.2f} Steps : {}\".format(i+1,score,steps))\n",
    "# Print stats at the end the run\n",
    "print('Mean of {} episodes = {}'.format(i+1,np.mean(scores)))\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _That's All Folks !!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
