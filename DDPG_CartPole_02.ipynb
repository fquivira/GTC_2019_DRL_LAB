{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On #11 : Balancing the Cart Pole w/ DDPG!\n",
    "---\n",
    "\n",
    "### Goal:\n",
    "- Implement DDPG on the CartPole Environment\n",
    "    * It is an overkill, but we will get a good understanding and also can compare against other algorithms\n",
    "    \n",
    "### Steps:\n",
    "1. Program DDPG Algorithm\n",
    "2. Run and Optimize\n",
    "3. Plot Values, like we did in other exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Organization\n",
    "#### The program has 3 parts :\n",
    "- Part 1 Defines the classes, initiates the environment and so forth. It sets up all the scaffolding needed\n",
    "- Part 2 Explore and Learn - it performs the DDPG Reinforcement Learning. It also saves the best model\n",
    "- Part 3 Run saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Definitions & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Install the required packages\n",
    "\n",
    "* No esoteric requirements\n",
    "* You can run them without docker\n",
    "* pip install -r requirements.txt\n",
    "* Requirements\n",
    " * python 3.6, pytorch, openAI gym, numpy, matplotlib\n",
    " * anaconda is easier but not needed\n",
    " * Miniconda works fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define imports\n",
    "\n",
    "python 3, numpy, matplotlib, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import copy\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants Definitions\n",
    "BUFFER_SIZE = 4096 # 2048 # 512 # int(1e5) # int(1e6) # int(1e5)  # replay buffer size  ?\n",
    "BATCH_SIZE = 32 # 64 # 32 # 128 # 64 # 256        # minibatch size for training\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 0.05 # 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 0.01 # 5e-4 # 1e-4 # 0.001 # 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 0.001 # 6e-4 # 3e-4  # 3e-3 # 0.001 # 3e-4        # learning rate of the critic 0.001\n",
    "WEIGHT_DECAY = 0.0001   # L2 weight decay\n",
    "# Number of neurons in the layers of the Actor & Critic Networks\n",
    "FC_UNITS_ACTOR = [16,8] # [4,4] # [32,16] #[400,300] #[8,8] #[128,128] # [64,128] # [32,16] # [400,300] # [128,128]\n",
    "FC_UNITS_CRITIC = [16,8]# [4,4] # [32,16] #[400,300] #[8,8] #[128,128] # [64,128] # [32,16] # [400,300] # [128,128]\n",
    "# Store models flag. Store during calibration runs and do not store during hyperparameter search\n",
    "# Used in Part 3 to run a stored model\n",
    "STORE_MODELS = False # True - Turn it on when you are ready to do the calibration training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym, PIL\n",
    "# env = gym.make('CartPole-v0')\n",
    "# array = env.reset()\n",
    "# PIL.Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Create instance & Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01258566, -0.00156614,  0.04207708, -0.00180545])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(42)\n",
    "# array = env.reset()\n",
    "env.reset()\n",
    "# ** render doesn't work reliably on a server. Uncomment when running ** locally **\n",
    "# env.render()\n",
    "#PIL.Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This what it will look like\n",
    "### We don't need the render(). We run it on headless mode and inspect the results\n",
    "<img src=\"CartPole_Render.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Examine the State and Action Spaces\n",
    "\n",
    "* The state space is continuous, with an observation space of 4 \n",
    "    * {x,$\\dot{x}$,$\\theta$, theta_dot}\n",
    "        * Cart Position,  Cart Velocity, Pole Angle, Pole Velocity at tip\n",
    "        * The angle, probably, is in radians\n",
    "\n",
    "The action space, on the contrary is simple viz. 0 = Left, 1 = Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n",
      "[0, 1]\n",
      "[ 0 = Left, 1 = Right ]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "act_space = [i for i in range(0,env.action_space.n)]\n",
    "print(act_space)\n",
    "# env.unwrapped.get_action_meanings() # AttributeError: 'FrozenLakeEnv' object has no attribute 'get_action_meanings'\n",
    "print('[ 0 = Left, 1 = Right ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_elapsed_seconds', '_elapsed_steps', '_episode_started_at', '_max_episode_seconds', '_max_episode_steps', '_past_limit', 'action_space', 'class_name', 'close', 'compute_reward', 'env', 'metadata', 'observation_space', 'render', 'reset', 'reward_range', 'seed', 'spec', 'step', 'unwrapped']\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'action_space', 'close', 'force_mag', 'gravity', 'kinematics_integrator', 'length', 'masscart', 'masspole', 'metadata', 'np_random', 'observation_space', 'polemass_length', 'render', 'reset', 'reward_range', 'seed', 'spec', 'state', 'step', 'steps_beyond_done', 'tau', 'theta_threshold_radians', 'total_mass', 'unwrapped', 'viewer', 'x_threshold']\n",
      "States =  Box(4,)\n",
      "Actions =  Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(dir(env))\n",
    "print(dir(env.unwrapped))\n",
    "# To see what functions and variables are availabe\n",
    "print('States = ',env.unwrapped.observation_space)\n",
    "print('Actions = ',env.unwrapped.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test the environment with Random Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [ 0.00560942  0.01842265 -0.03590751 -0.0120678 ] ] -> 0  : [ [ 0.00597787 -0.17616644 -0.03614886  0.26907314] ] R= 1.0\n",
      "[ [ 0.00597787 -0.17616644 -0.03614886  0.26907314] ] -> 1  : [ [ 0.00245454  0.01945224 -0.0307674  -0.03478864] ] R= 1.0\n",
      "[ [ 0.00245454  0.01945224 -0.0307674  -0.03478864] ] -> 1  : [ [ 0.00284359  0.21500159 -0.03146317 -0.33701802] ] R= 1.0\n",
      "[ [ 0.00284359  0.21500159 -0.03146317 -0.33701802] ] -> 0  : [ [ 0.00714362  0.02034118 -0.03820353 -0.05442056] ] R= 1.0\n",
      "[ [ 0.00714362  0.02034118 -0.03820353 -0.05442056] ] -> 1  : [ [ 0.00755044  0.2159895  -0.03929194 -0.35890801] ] R= 1.0\n",
      "[ [ 0.00755044  0.2159895  -0.03929194 -0.35890801] ] -> 1  : [ [ 0.01187023  0.41164734 -0.0464701  -0.66371712] ] R= 1.0\n",
      "[ [ 0.01187023  0.41164734 -0.0464701  -0.66371712] ] -> 1  : [ [ 0.02010318  0.60738391 -0.05974445 -0.97066231] ] R= 1.0\n",
      "[ [ 0.02010318  0.60738391 -0.05974445 -0.97066231] ] -> 1  : [ [ 0.03225086  0.80325467 -0.07915769 -1.28149866] ] R= 1.0\n",
      "[ [ 0.03225086  0.80325467 -0.07915769 -1.28149866] ] -> 1  : [ [ 0.04831595  0.99929074 -0.10478767 -1.59788005] ] R= 1.0\n",
      "[ [ 0.04831595  0.99929074 -0.10478767 -1.59788005] ] -> 1  : [ [ 0.06830176  1.19548692 -0.13674527 -1.92131128] ] R= 1.0\n",
      "[ [ 0.06830176  1.19548692 -0.13674527 -1.92131128] ] -> 1  : [ [ 0.0922115   1.39178778 -0.17517149 -2.25309178] ] R= 1.0\n",
      "[ [ 0.0922115   1.39178778 -0.17517149 -2.25309178] ] -> 0  : [ [ 0.12004726  1.19869297 -0.22023333 -2.0191195 ] ] R= 1.0\n",
      "Episode 1 finished after 12 steps with a Total Reward = 12\n",
      "[ [-0.03056663  0.02063487  0.01650017  0.04869769] ] -> 0  : [ [-0.03015393 -0.17471974  0.01747412  0.34654056] ] R= 1.0\n",
      "[ [-0.03015393 -0.17471974  0.01747412  0.34654056] ] -> 1  : [ [-0.03364832  0.02014935  0.02440493  0.05941868] ] R= 1.0\n",
      "[ [-0.03364832  0.02014935  0.02440493  0.05941868] ] -> 0  : [ [-0.03324534 -0.17531386  0.02559331  0.35970053] ] R= 1.0\n",
      "[ [-0.03324534 -0.17531386  0.02559331  0.35970053] ] -> 0  : [ [-0.03675161 -0.3707901   0.03278732  0.66034247] ] R= 1.0\n",
      "[ [-0.03675161 -0.3707901   0.03278732  0.66034247] ] -> 0  : [ [-0.04416742 -0.56635261  0.04599417  0.96316632] ] R= 1.0\n",
      "[ [-0.04416742 -0.56635261  0.04599417  0.96316632] ] -> 0  : [ [-0.05549447 -0.76206137  0.06525749  1.26993652] ] R= 1.0\n",
      "[ [-0.05549447 -0.76206137  0.06525749  1.26993652] ] -> 0  : [ [-0.0707357  -0.957953    0.09065622  1.58232061] ] R= 1.0\n",
      "[ [-0.0707357  -0.957953    0.09065622  1.58232061] ] -> 1  : [ [-0.08989476 -0.76401924  0.12230263  1.31923099] ] R= 1.0\n",
      "[ [-0.08989476 -0.76401924  0.12230263  1.31923099] ] -> 0  : [ [-0.10517514 -0.96045669  0.14868725  1.64755357] ] R= 1.0\n",
      "[ [-0.10517514 -0.96045669  0.14868725  1.64755357] ] -> 1  : [ [-0.12438427 -0.76735361  0.18163833  1.40464805] ] R= 1.0\n",
      "[ [-0.12438427 -0.76735361  0.18163833  1.40464805] ] -> 1  : [ [-0.13973135 -0.57489151  0.20973129  1.17381268] ] R= 1.0\n",
      "Episode 2 finished after 11 steps with a Total Reward = 11\n",
      "[ [-0.0313163   0.03775876 -0.01135978 -0.01095196] ] -> 0  : [ [-0.03056113 -0.15719845 -0.01157882  0.27812529] ] R= 1.0\n",
      "[ [-0.03056113 -0.15719845 -0.01157882  0.27812529] ] -> 0  : [ [-0.0337051  -0.35215332 -0.00601632  0.56713389] ] R= 1.0\n",
      "[ [-0.0337051  -0.35215332 -0.00601632  0.56713389] ] -> 1  : [ [-0.04074816 -0.15694749  0.00532636  0.27256166] ] R= 1.0\n",
      "[ [-0.04074816 -0.15694749  0.00532636  0.27256166] ] -> 1  : [ [-0.04388711  0.03809805  0.01077759 -0.01843656] ] R= 1.0\n",
      "[ [-0.04388711  0.03809805  0.01077759 -0.01843656] ] -> 1  : [ [-0.04312515  0.2330638   0.01040886 -0.30769964] ] R= 1.0\n",
      "[ [-0.04312515  0.2330638   0.01040886 -0.30769964] ] -> 1  : [ [-0.03846388  0.4280359   0.00425487 -0.5970818 ] ] R= 1.0\n",
      "[ [-0.03846388  0.4280359   0.00425487 -0.5970818 ] ] -> 0  : [ [-0.02990316  0.23285466 -0.00768677 -0.30306167] ] R= 1.0\n",
      "[ [-0.02990316  0.23285466 -0.00768677 -0.30306167] ] -> 1  : [ [-0.02524607  0.42808532 -0.013748   -0.59815889] ] R= 1.0\n",
      "[ [-0.02524607  0.42808532 -0.013748   -0.59815889] ] -> 0  : [ [-0.01668436  0.2331584  -0.02571118 -0.30983793] ] R= 1.0\n",
      "[ [-0.01668436  0.2331584  -0.02571118 -0.30983793] ] -> 1  : [ [-0.01202119  0.42863707 -0.03190794 -0.61051727] ] R= 1.0\n",
      "[ [-0.01202119  0.42863707 -0.03190794 -0.61051727] ] -> 0  : [ [-0.00344845  0.23397531 -0.04411828 -0.3280526 ] ] R= 1.0\n",
      "[ [-0.00344845  0.23397531 -0.04411828 -0.3280526 ] ] -> 1  : [ [ 0.00123106  0.42969667 -0.05067933 -0.63431554] ] R= 1.0\n",
      "[ [ 0.00123106  0.42969667 -0.05067933 -0.63431554] ] -> 1  : [ [ 0.00982499  0.62548754 -0.06336564 -0.94251813] ] R= 1.0\n",
      "[ [ 0.00982499  0.62548754 -0.06336564 -0.94251813] ] -> 0  : [ [ 0.02233474  0.43127402 -0.08221601 -0.67039954] ] R= 1.0\n",
      "[ [ 0.02233474  0.43127402 -0.08221601 -0.67039954] ] -> 1  : [ [ 0.03096022  0.62743695 -0.095624   -0.98779431] ] R= 1.0\n",
      "[ [ 0.03096022  0.62743695 -0.095624   -0.98779431] ] -> 1  : [ [ 0.04350896  0.82370014 -0.11537988 -1.30891479] ] R= 1.0\n",
      "[ [ 0.04350896  0.82370014 -0.11537988 -1.30891479] ] -> 0  : [ [ 0.05998296  0.63021347 -0.14155818 -1.05446096] ] R= 1.0\n",
      "[ [ 0.05998296  0.63021347 -0.14155818 -1.05446096] ] -> 0  : [ [ 0.07258723  0.4372227  -0.1626474  -0.80934968] ] R= 1.0\n",
      "[ [ 0.07258723  0.4372227  -0.1626474  -0.80934968] ] -> 1  : [ [ 0.08133169  0.634155   -0.17883439 -1.14845725] ] R= 1.0\n",
      "[ [ 0.08133169  0.634155   -0.17883439 -1.14845725] ] -> 0  : [ [ 0.09401479  0.44176002 -0.20180354 -0.91676484] ] R= 1.0\n",
      "[ [ 0.09401479  0.44176002 -0.20180354 -0.91676484] ] -> 1  : [ [ 0.10284999  0.63895416 -0.22013883 -1.26548183] ] R= 1.0\n",
      "Episode 3 finished after 21 steps with a Total Reward = 21\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(3):\n",
    "    state = env.reset()\n",
    "    tot_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        print('[',state,']','->', action,' : [',next_state,']', 'R=',reward)\n",
    "        # env.render()\n",
    "        tot_reward += reward\n",
    "        steps += 1\n",
    "        if done:\n",
    "            print('Episode {:d} finished after {:d} steps with a Total Reward = {:.0f}'.\n",
    "                  format(i_episode+1,steps, tot_reward))\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "# Pole angle +/-12 degrees, Cart Pos +/- 2.4 or 200 steps\n",
    "# Cart Pos, Velocity, Pole Angle, Velocity\n",
    "# 12 degrees = .2094 radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device = {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Algorithm\n",
    "\n",
    "We are using the DDPG. I liked the simple systems diagram that Prof. Sergey Levine from UC Berkeley uses for his CS294 Deep Reinforcement Learning class[http://rail.eecs.berkeley.edu/deeprlcourse/]\n",
    "<img src=\"RL_Systems_Flow.png\">\n",
    "\n",
    "The major components of the algorithm are:\n",
    "1. `Actor` implemented as a Deep Neural Network whih consists of fully connected layers.\n",
    "2. `Critic` implemented as a Deep Neural Network which consists of fully connected networks\n",
    "3. `Experience replay buffer` - in order to train the network we take actions and then store the results in the replay buffer. The replay buffer is a circular buffer and it has methods to sample a random batch\n",
    "3. `The Agent` brings all of the above together. It interacts with the environment by taking actions based on a policy, collects rewards and the observation feedback, then stores the experience in the replay buffer and also initiates a learning step on the actor and critic networks\n",
    "     * The agent has 3 main components viz:\n",
    "         1. The DDPG Orchestrator which interacts with the environment by taking actions and then unpacking the returned package to rewards, state space et al.\n",
    "         2. It also has to do housekeeping like tracking scores, store high performant models and check when the problem is solved\n",
    "         3. The 3rd component is the most interesting one - it gives the agent the capability to hunt for the right policy !!\n",
    "\n",
    "### Pseudo Code\n",
    "<img src=\"DDPG_Alg.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size=state_size, action_size=action_size, seed=42, fc_units=FC_UNITS_ACTOR):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state, defaults to the global state size from the env\n",
    "            action_size (int): Dimension of each action, defaults to the global action size from the env\n",
    "            seed (int): Random seed\n",
    "            fc_units (list(int)): Number of nodes in the hidden layers as a list\n",
    "            ** Hard coded as a 3 layer network \n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            #nn.BatchNorm1d(state_size),\n",
    "            nn.Linear(state_size,fc_units[0]),\n",
    "            nn.ReLU(),\n",
    "            #nn.BatchNorm1d(fc_units[0]),\n",
    "            nn.Linear(fc_units[0],fc_units[1]),\n",
    "            nn.ReLU(),\n",
    "            #nn.BatchNorm1d(fc_units[1]),\n",
    "            nn.Linear(fc_units[1],action_size),\n",
    "            # nn.Tanh() # for continuous -1 to +1\n",
    "            nn.Softmax(dim=-1)\n",
    "            # nn.Sigmoid() # for 0-1\n",
    "        )\n",
    "        self.model.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self,m):\n",
    "        if (type(m) == nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            #nn.init.xavier_normal_(m.weight)\n",
    "            # nn.init.kaiming_normal_(m.weight)\n",
    "            m.bias.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        return self.model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size = state_size, action_size = action_size, seed=42, fc_units=FC_UNITS_CRITIC):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state, defaults to the global state size from the env\n",
    "            action_size (int): Dimension of each action, defaults to the global action size from the env\n",
    "            seed (int): Random seed\n",
    "            fc_units (list(int)): Number of nodes in the hidden layers as a list\n",
    "            ** Hard coded as a 3 layer network \n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.hc_1 = nn.Sequential(\n",
    "            nn.Linear(state_size,fc_units[0]),\n",
    "            nn.ReLU(), # leaky relu ?\n",
    "            # nn.BatchNorm1d(fc_units[0])\n",
    "        )\n",
    "        self.hc_2 = nn.Sequential(\n",
    "            nn.Linear(fc_units[0]+action_size,fc_units[1]),\n",
    "            nn.ReLU(), # leaky relu ?\n",
    "            nn.Linear(fc_units[1],1)\n",
    "        )\n",
    "        # Initialize the layers\n",
    "        self.hc_1.apply(self.init_weights)\n",
    "        self.hc_2.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = self.hc_1(state)\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = self.hc_2(x)\n",
    "        return (x)\n",
    "    \n",
    "    def init_weights(self,layer):\n",
    "        if (type(layer) == nn.Linear):\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            layer.bias.data.fill_(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed=42):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=False): # True\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        #print(state)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return action # np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6. Instantiate an agent\n",
    "\n",
    "The state space and the action space dimensions come from the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=2, bias=True)\n",
      "    (5): Softmax()\n",
      "  )\n",
      ")\n",
      "Critic(\n",
      "  (hc_1): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hc_2): Sequential(\n",
      "    (0): Linear(in_features=18, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=42)\n",
    "print(agent.actor_local)\n",
    "print(agent.critic_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Learn & Train\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. DDPG Algorithm\n",
    "\n",
    "Define the DDPG Algorithm. Once we have defined the foundations (network, buffer, actor, critic, agent and so forth), the DDPG is relatively easy. It has a few responsibilities:\n",
    "1. Orchastrate the episodes calling the appropriate methods\n",
    "2. Display a running commentry of the scores and episode count\n",
    "3. Check the success criterion for solving the environment i.e. if running average is > 195 and print the episode count\n",
    "4. Store the model with the maximum score\n",
    "5. Keep track of the scores for analytics at the end of the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=1000):\n",
    "    scores_window = deque(maxlen=100)\n",
    "    scores = []\n",
    "    score  = 0\n",
    "    max_score = -np.Inf\n",
    "    has_seen_195 = False\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()                    # reset the environment    \n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        max_steps = 0\n",
    "        while True:\n",
    "            action = agent.act(state) # treat as softmax probabilities\n",
    "            act = int(np.random.choice(action_size, p=action)) # for Softmax\n",
    "            # act = 1 if action > 0.5 else 0 # for Sigmoid\n",
    "            next_state, reward, done, _ = env.step(act)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "            max_steps += 1\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:6.2f}\\tScore: {:6.2f}\\tMax_steps : {:3d}'.\\\n",
    "              format(i_episode, np.mean(scores_window), score, max_steps), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))  \n",
    "        if (np.mean(scores_window) >= 195.0) and (not has_seen_195):\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:5.2f}'.\\\n",
    "                  format(i_episode-100, np.mean(scores_window)))\n",
    "            # torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            has_seen_195 = True\n",
    "            break # Early stop\n",
    "            # To see how far it can go :\n",
    "            # comment the break out. The has_seen_195 will stop printing the \"Environment Solved\" every time\n",
    "        # Store the best model if desired\n",
    "        if STORE_MODELS:\n",
    "            if np.mean(scores_window) > max_score:\n",
    "                max_score = np.mean(scores_window)\n",
    "                torch.save(agent, 'checkpoint.pth')\n",
    "                # print(' .. Storing with score {}'.format(max_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. The actual training Run\n",
    "\n",
    "1. Run the DDPG\n",
    "2. Calculate and display end-of-run analytics viz. descriptive statistics and a plot of the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 14.377\tScore:  50.00\tMax_steps :  49\n",
      "Episode 200\tAverage Score: 162.22\tScore: 200.00\tMax_steps : 199\n",
      "Episode 230\tAverage Score: 195.04\tScore: 200.00\tMax_steps : 199\n",
      "Environment solved in 130 episodes!\tAverage Score: 195.04\n",
      "Elapsed : 0:01:16.801558\n",
      "2019-02-08 12:59:29.923779\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=2, bias=True)\n",
      "    (5): Softmax()\n",
      "  )\n",
      ")\n",
      "Max Score 200.00 at 133\n",
      "Percentile [25,50,75] : [ 10.   94.5 200. ]\n",
      "Variance : 7377.334\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "scores = ddpg(n_episodes=1500) # 2000,1500, 1000 ; For quick functional test use 100,500\n",
    "env.close() # Close the environment\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())\n",
    "#\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "print(agent.actor_local)\n",
    "# print(agent.critic_local)\n",
    "print('Max Score {:.2f} at {}'.format(np.max(scores), np.argmax(scores)))\n",
    "print('Percentile [25,50,75] : {}'.format(np.percentile(scores,[25,50,75])))\n",
    "print('Variance : {:.3f}'.format(np.var(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Run\n",
    "#### _In 130 episodes !_\n",
    "<img src=\"ddpg_run_02.png\">\n",
    "<img src=\"ddpg_run_01.png\">\n",
    "<img src=\"ddpg_run.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Run logs & Notes\n",
    "\n",
    "1. A place to keep the statistics and qualitative observations\n",
    "2. It is easier to keep notes as soon as a run is done\n",
    "3. Also a place to keep future explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs\n",
    "#### 1/13/19\n",
    "\n",
    "1. Tried Softmax for actor, tried Sigmoid (with action_size=1) - was tanh()\n",
    "    * LR : Actor = 0.01, Critic = 0.001\n",
    "    * network 8 X 8\n",
    "1. Doesn't go beyond 10-11 steps when episode = 1500, buffer = 512\n",
    "2. No clue when it will wake up - episode = 15000; no dice so far\n",
    "3. Buffer = 2048 no change\n",
    "4. Took out batchnorm\n",
    "5. Back to softmax\n",
    "6. Took out clip(-1,1)\n",
    "    * No change score ~10\n",
    "7. tau = 0.05 - started learning - Am seeing the scores in the 20s! See if it passes nope stays at 20s\n",
    "8. network 400 X 300 ! goes back to a score of 10\n",
    "1. Network 36 X 8 ! Nope no good\n",
    "1. Network 4 X 4 - very small !\n",
    "1. Buffer = 4096, Batch = 32 meanders around a score of 10 for 10000 episodes !\n",
    "1. softmax(-1) definite progress - goes upto 200, but then gets 10 as well ! After 1100, it is steady !\n",
    " * Solved in 1029 episodes !\n",
    " * Was about to throw the towel ! Took me a day ! Was going to go back to A2C !\n",
    " * From 1300 onwards it gets prfect 200\n",
    " * __added noise and that saved the day !__\n",
    "1. back to 16 X 8 Network, batch = 64, 1500 episodes (was 15,000 ! and didn't do any good. The progress of a learning network is evident)\n",
    " * would have solved under 300, but a few black sheep epidodes bring the average down ;o(\n",
    " * 860 episodes. Between 300 and 800, it went down and came back !\n",
    "1. Cleaned up a little bit and ran again - 282 Episodes !\n",
    "\n",
    "#### 2/2/19\n",
    "\n",
    "1. Again got into a rut-nothing beyond a score of 11\n",
    "2. Changd network to FC4-FC4\n",
    "3. Better but - couldn't solve with 1000 episodes, learns and then unlearns\n",
    "4. Increasing episodes back to 15000. Nope\n",
    "5. Batch = 32; good steady progress. Woould it make it ? Nope\n",
    "6. Noise = True, was False for some reason. won't work for softmax\n",
    "7. Network FC16-FC8 - solved in 533 Steps !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Run a stored Model or the learned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Here we are saving and loading the whole model. We cal also save & load the state dict\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from the saved file\n",
    "# The file has the parameters of the model that has the highest score during training\n",
    "# agent = torch.load('checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :  1 Score : 200.00 Steps : 199\n",
      "Episode :  2 Score : 200.00 Steps : 199\n",
      "Episode :  3 Score : 200.00 Steps : 199\n",
      "Episode :  4 Score : 200.00 Steps : 199\n",
      "Episode :  5 Score : 200.00 Steps : 199\n",
      "Episode :  6 Score : 200.00 Steps : 199\n",
      "Episode :  7 Score : 200.00 Steps : 199\n",
      "Episode :  8 Score : 200.00 Steps : 199\n",
      "Episode :  9 Score : 200.00 Steps : 199\n",
      "Episode : 10 Score : 200.00 Steps : 199\n",
      "Mean of 10 episodes = 200.0\n",
      "Elapsed : 0:00:00.463654\n",
      "2019-02-08 13:02:39.850664\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "scores=[]\n",
    "for i in range(10): # 10 episodes\n",
    "    state = env.reset()                                # reset the environment & get the current state\n",
    "    score = 0                                          # initialize the score\n",
    "    steps = 0                                          # Keep track of the number of steps\n",
    "    while True:\n",
    "        action = agent.act(state)                      # select an action, treat as softmax probabilities\n",
    "        act = int(np.random.choice(action_size, p=action)) # for Softmax\n",
    "        next_state, reward, done, _ = env.step(act)    # send the action to the environment\n",
    "        score += reward                                # update the score\n",
    "        state = next_state                             # roll over the state to next time step\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            break\n",
    "        else:\n",
    "            steps += 1\n",
    "    scores.append(score)\n",
    "    print(\"Episode : {:2d} Score : {:5.2f} Steps : {}\".format(i+1,score,steps))\n",
    "# Print stats at the end the run\n",
    "print('Mean of {} episodes = {}'.format(i+1,np.mean(scores)))\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _That's all Folks !!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
