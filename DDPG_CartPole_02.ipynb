{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On #11 : Balancing the Cart Pole w/ DDPG!\n",
    "---\n",
    "\n",
    "### Goal:\n",
    "- Implement DDPG on the CartPole Environment\n",
    "    * It is an overkill, but we will get a good understanding and also can compare against other algorithms\n",
    "    \n",
    "### Steps:\n",
    "1. Program DDPG Algorithm\n",
    "2. Run and Optimize\n",
    "3. Plot Values, like we did in other exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Organization\n",
    "#### The program has 3 parts :\n",
    "- Part 1 Defines the classes, initiates the environment and so forth. It sets up all the scaffolding needed\n",
    "- Part 2 Explore and Learn - it performs the DDPG Reinforcement Learning. It also saves the best model\n",
    "- Part 3 Run saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Definitions & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Install the required packages\n",
    "\n",
    "The required setup is detailed in the README.md\n",
    "\n",
    "I am running this on a MacBookPro 14,3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define imports\n",
    "\n",
    "python 3, numpy, matplotlib, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import copy\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants Definitions\n",
    "BUFFER_SIZE = 4096 # 2048 # 512 # int(1e5) # int(1e6) # int(1e5)  # replay buffer size  ?\n",
    "BATCH_SIZE = 64 # 32 # 128 # 64 # 256        # minibatch size for training\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 0.05 # 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 0.01 # 5e-4 # 1e-4 # 0.001 # 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 0.001 # 6e-4 # 3e-4  # 3e-3 # 0.001 # 3e-4        # learning rate of the critic 0.001\n",
    "WEIGHT_DECAY = 0.0001   # L2 weight decay\n",
    "# Number of neurons in the layers of the Actor & Critic Networks\n",
    "FC_UNITS_ACTOR = [16,8] # [4,4] # [32,16] #[400,300] #[8,8] #[128,128] # [64,128] # [32,16] # [400,300] # [128,128]\n",
    "FC_UNITS_CRITIC = [16,8] #[4,4] # [32,16] #[400,300] #[8,8] #[128,128] # [64,128] # [32,16] # [400,300] # [128,128]\n",
    "# Store models flag. Store during calibration runs and do not store during hyperparameter search\n",
    "# Used in Part 3 to run a stored model\n",
    "STORE_MODELS = False # True - Turn it on when you are ready to do the calibration training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import gym, PIL\\nenv = gym.make('CartPole-v0')\\narray = env.reset()\\nPIL.Image.fromarray(env.render(mode='rgb_array'))\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import gym, PIL\n",
    "env = gym.make('CartPole-v0')\n",
    "array = env.reset()\n",
    "PIL.Image.fromarray(env.render(mode='rgb_array'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Create instance & Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01258566, -0.00156614,  0.04207708, -0.00180545])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(42)\n",
    "# array = env.reset()\n",
    "env.reset()\n",
    "# env.render()\n",
    "#PIL.Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Examine the State and Action Spaces\n",
    "\n",
    "* The state space is continuous, with an observation space of 4 \n",
    "    * {x,$\\dot{x}$,$\\theta$, theta_dot}\n",
    "        * Cart Position,  Cart Velocity, Pole Angle, Pole Velocity at tip\n",
    "        * The angle, probably, is in radians\n",
    "\n",
    "The action space, on the contrary is simple viz. 0 = Left, 1 = Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n",
      "[0, 1]\n",
      "[ 0 = Left, 1 = Right ]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "act_space = [i for i in range(0,env.action_space.n)]\n",
    "print(act_space)\n",
    "# env.unwrapped.get_action_meanings() # AttributeError: 'FrozenLakeEnv' object has no attribute 'get_action_meanings'\n",
    "print('[ 0 = Left, 1 = Right ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_elapsed_seconds', '_elapsed_steps', '_episode_started_at', '_max_episode_seconds', '_max_episode_steps', '_past_limit', 'action_space', 'class_name', 'close', 'compute_reward', 'env', 'metadata', 'observation_space', 'render', 'reset', 'reward_range', 'seed', 'spec', 'step', 'unwrapped']\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'action_space', 'close', 'force_mag', 'gravity', 'kinematics_integrator', 'length', 'masscart', 'masspole', 'metadata', 'np_random', 'observation_space', 'polemass_length', 'render', 'reset', 'reward_range', 'seed', 'spec', 'state', 'step', 'steps_beyond_done', 'tau', 'theta_threshold_radians', 'total_mass', 'unwrapped', 'viewer', 'x_threshold']\n",
      "States =  Box(4,)\n",
      "Actions =  Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(dir(env))\n",
    "print(dir(env.unwrapped))\n",
    "# To see what functions and variables are availabe\n",
    "print('States = ',env.unwrapped.observation_space)\n",
    "print('Actions = ',env.unwrapped.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test the environment with Random Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [ 0.00560942  0.01842265 -0.03590751 -0.0120678 ] ] -> 0  : [ [ 0.00597787 -0.17616644 -0.03614886  0.26907314] ] R= 1.0\n",
      "[ [ 0.00597787 -0.17616644 -0.03614886  0.26907314] ] -> 1  : [ [ 0.00245454  0.01945224 -0.0307674  -0.03478864] ] R= 1.0\n",
      "[ [ 0.00245454  0.01945224 -0.0307674  -0.03478864] ] -> 1  : [ [ 0.00284359  0.21500159 -0.03146317 -0.33701802] ] R= 1.0\n",
      "[ [ 0.00284359  0.21500159 -0.03146317 -0.33701802] ] -> 0  : [ [ 0.00714362  0.02034118 -0.03820353 -0.05442056] ] R= 1.0\n",
      "[ [ 0.00714362  0.02034118 -0.03820353 -0.05442056] ] -> 1  : [ [ 0.00755044  0.2159895  -0.03929194 -0.35890801] ] R= 1.0\n",
      "[ [ 0.00755044  0.2159895  -0.03929194 -0.35890801] ] -> 1  : [ [ 0.01187023  0.41164734 -0.0464701  -0.66371712] ] R= 1.0\n",
      "[ [ 0.01187023  0.41164734 -0.0464701  -0.66371712] ] -> 1  : [ [ 0.02010318  0.60738391 -0.05974445 -0.97066231] ] R= 1.0\n",
      "[ [ 0.02010318  0.60738391 -0.05974445 -0.97066231] ] -> 1  : [ [ 0.03225086  0.80325467 -0.07915769 -1.28149866] ] R= 1.0\n",
      "[ [ 0.03225086  0.80325467 -0.07915769 -1.28149866] ] -> 1  : [ [ 0.04831595  0.99929074 -0.10478767 -1.59788005] ] R= 1.0\n",
      "[ [ 0.04831595  0.99929074 -0.10478767 -1.59788005] ] -> 1  : [ [ 0.06830176  1.19548692 -0.13674527 -1.92131128] ] R= 1.0\n",
      "[ [ 0.06830176  1.19548692 -0.13674527 -1.92131128] ] -> 1  : [ [ 0.0922115   1.39178778 -0.17517149 -2.25309178] ] R= 1.0\n",
      "[ [ 0.0922115   1.39178778 -0.17517149 -2.25309178] ] -> 0  : [ [ 0.12004726  1.19869297 -0.22023333 -2.0191195 ] ] R= 1.0\n",
      "Episode 1 finished after 12 steps with a Total Reward = 12\n",
      "[ [-0.03056663  0.02063487  0.01650017  0.04869769] ] -> 0  : [ [-0.03015393 -0.17471974  0.01747412  0.34654056] ] R= 1.0\n",
      "[ [-0.03015393 -0.17471974  0.01747412  0.34654056] ] -> 1  : [ [-0.03364832  0.02014935  0.02440493  0.05941868] ] R= 1.0\n",
      "[ [-0.03364832  0.02014935  0.02440493  0.05941868] ] -> 0  : [ [-0.03324534 -0.17531386  0.02559331  0.35970053] ] R= 1.0\n",
      "[ [-0.03324534 -0.17531386  0.02559331  0.35970053] ] -> 0  : [ [-0.03675161 -0.3707901   0.03278732  0.66034247] ] R= 1.0\n",
      "[ [-0.03675161 -0.3707901   0.03278732  0.66034247] ] -> 0  : [ [-0.04416742 -0.56635261  0.04599417  0.96316632] ] R= 1.0\n",
      "[ [-0.04416742 -0.56635261  0.04599417  0.96316632] ] -> 0  : [ [-0.05549447 -0.76206137  0.06525749  1.26993652] ] R= 1.0\n",
      "[ [-0.05549447 -0.76206137  0.06525749  1.26993652] ] -> 0  : [ [-0.0707357  -0.957953    0.09065622  1.58232061] ] R= 1.0\n",
      "[ [-0.0707357  -0.957953    0.09065622  1.58232061] ] -> 1  : [ [-0.08989476 -0.76401924  0.12230263  1.31923099] ] R= 1.0\n",
      "[ [-0.08989476 -0.76401924  0.12230263  1.31923099] ] -> 0  : [ [-0.10517514 -0.96045669  0.14868725  1.64755357] ] R= 1.0\n",
      "[ [-0.10517514 -0.96045669  0.14868725  1.64755357] ] -> 1  : [ [-0.12438427 -0.76735361  0.18163833  1.40464805] ] R= 1.0\n",
      "[ [-0.12438427 -0.76735361  0.18163833  1.40464805] ] -> 1  : [ [-0.13973135 -0.57489151  0.20973129  1.17381268] ] R= 1.0\n",
      "Episode 2 finished after 11 steps with a Total Reward = 11\n",
      "[ [-0.0313163   0.03775876 -0.01135978 -0.01095196] ] -> 0  : [ [-0.03056113 -0.15719845 -0.01157882  0.27812529] ] R= 1.0\n",
      "[ [-0.03056113 -0.15719845 -0.01157882  0.27812529] ] -> 0  : [ [-0.0337051  -0.35215332 -0.00601632  0.56713389] ] R= 1.0\n",
      "[ [-0.0337051  -0.35215332 -0.00601632  0.56713389] ] -> 1  : [ [-0.04074816 -0.15694749  0.00532636  0.27256166] ] R= 1.0\n",
      "[ [-0.04074816 -0.15694749  0.00532636  0.27256166] ] -> 1  : [ [-0.04388711  0.03809805  0.01077759 -0.01843656] ] R= 1.0\n",
      "[ [-0.04388711  0.03809805  0.01077759 -0.01843656] ] -> 1  : [ [-0.04312515  0.2330638   0.01040886 -0.30769964] ] R= 1.0\n",
      "[ [-0.04312515  0.2330638   0.01040886 -0.30769964] ] -> 1  : [ [-0.03846388  0.4280359   0.00425487 -0.5970818 ] ] R= 1.0\n",
      "[ [-0.03846388  0.4280359   0.00425487 -0.5970818 ] ] -> 0  : [ [-0.02990316  0.23285466 -0.00768677 -0.30306167] ] R= 1.0\n",
      "[ [-0.02990316  0.23285466 -0.00768677 -0.30306167] ] -> 1  : [ [-0.02524607  0.42808532 -0.013748   -0.59815889] ] R= 1.0\n",
      "[ [-0.02524607  0.42808532 -0.013748   -0.59815889] ] -> 0  : [ [-0.01668436  0.2331584  -0.02571118 -0.30983793] ] R= 1.0\n",
      "[ [-0.01668436  0.2331584  -0.02571118 -0.30983793] ] -> 1  : [ [-0.01202119  0.42863707 -0.03190794 -0.61051727] ] R= 1.0\n",
      "[ [-0.01202119  0.42863707 -0.03190794 -0.61051727] ] -> 0  : [ [-0.00344845  0.23397531 -0.04411828 -0.3280526 ] ] R= 1.0\n",
      "[ [-0.00344845  0.23397531 -0.04411828 -0.3280526 ] ] -> 1  : [ [ 0.00123106  0.42969667 -0.05067933 -0.63431554] ] R= 1.0\n",
      "[ [ 0.00123106  0.42969667 -0.05067933 -0.63431554] ] -> 1  : [ [ 0.00982499  0.62548754 -0.06336564 -0.94251813] ] R= 1.0\n",
      "[ [ 0.00982499  0.62548754 -0.06336564 -0.94251813] ] -> 0  : [ [ 0.02233474  0.43127402 -0.08221601 -0.67039954] ] R= 1.0\n",
      "[ [ 0.02233474  0.43127402 -0.08221601 -0.67039954] ] -> 1  : [ [ 0.03096022  0.62743695 -0.095624   -0.98779431] ] R= 1.0\n",
      "[ [ 0.03096022  0.62743695 -0.095624   -0.98779431] ] -> 1  : [ [ 0.04350896  0.82370014 -0.11537988 -1.30891479] ] R= 1.0\n",
      "[ [ 0.04350896  0.82370014 -0.11537988 -1.30891479] ] -> 0  : [ [ 0.05998296  0.63021347 -0.14155818 -1.05446096] ] R= 1.0\n",
      "[ [ 0.05998296  0.63021347 -0.14155818 -1.05446096] ] -> 0  : [ [ 0.07258723  0.4372227  -0.1626474  -0.80934968] ] R= 1.0\n",
      "[ [ 0.07258723  0.4372227  -0.1626474  -0.80934968] ] -> 1  : [ [ 0.08133169  0.634155   -0.17883439 -1.14845725] ] R= 1.0\n",
      "[ [ 0.08133169  0.634155   -0.17883439 -1.14845725] ] -> 0  : [ [ 0.09401479  0.44176002 -0.20180354 -0.91676484] ] R= 1.0\n",
      "[ [ 0.09401479  0.44176002 -0.20180354 -0.91676484] ] -> 1  : [ [ 0.10284999  0.63895416 -0.22013883 -1.26548183] ] R= 1.0\n",
      "Episode 3 finished after 21 steps with a Total Reward = 21\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(3):\n",
    "    state = env.reset()\n",
    "    tot_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        print('[',state,']','->', action,' : [',next_state,']', 'R=',reward)\n",
    "        # env.render()\n",
    "        tot_reward += reward\n",
    "        steps += 1\n",
    "        if done:\n",
    "            print('Episode {:d} finished after {:d} steps with a Total Reward = {:.0f}'.\n",
    "                  format(i_episode+1,steps, tot_reward))\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "# Pole angle +/-12 degrees, Cart Pos +/- 2.4 or 200 steps\n",
    "# Cart Pos, Velocity, Pole Angle, Velocity\n",
    "# 12 degrees = .2094 radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device = {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Algorithm\n",
    "\n",
    "We are using the DDPG. I liked the simple systems diagram that Prof. Sergey Levine from UC Berkeley uses for his CS294 Deep Reinforcement Learning class[http://rail.eecs.berkeley.edu/deeprlcourse/]\n",
    "<img src=\"RL_Systems_Flow.png\">\n",
    "\n",
    "The major components of the algorithm are:\n",
    "1. `Actor` implemented as a Deep Neural Network whih consists of fully connected layers.\n",
    "2. `Critic` implemented as a Deep Neural Network which consists of fully connected networks\n",
    "3. `Experience replay buffer` - in order to train the network we take actions and then store the results in the replay buffer. The replay buffer is a circular buffer and it has methods to sample a random batch\n",
    "3. `The Agent` brings all of the above together. It interacts with the environment by taking actions based on a policy, collects rewards and the observation feedback, then stores the experience in the replay buffer and also initiates a learning step on the actor and critic networks\n",
    "     * The agent has 3 main components viz:\n",
    "         1. The DDPG Orchestrator which interacts with the environment by taking actions and then unpacking the returned package to rewards, state space et al.\n",
    "         2. It also has to do housekeeping like tracking scores, store high performant models and check when the problem is solved\n",
    "         3. The 3rd component is the most interesting one - it gives the agent the capability to hunt for the right policy !!\n",
    "\n",
    "### Pseudo Code\n",
    "<img src=\"DDPG_Alg.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size=state_size, action_size=action_size, seed=42, fc_units=FC_UNITS_ACTOR):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state, defaults to the global state size from the env\n",
    "            action_size (int): Dimension of each action, defaults to the global action size from the env\n",
    "            seed (int): Random seed\n",
    "            fc_units (list(int)): Number of nodes in the hidden layers as a list\n",
    "            ** Hard coded as a 3 layer network \n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            #nn.BatchNorm1d(state_size),\n",
    "            nn.Linear(state_size,fc_units[0]),\n",
    "            nn.ReLU(),\n",
    "            #nn.BatchNorm1d(fc_units[0]),\n",
    "            nn.Linear(fc_units[0],fc_units[1]),\n",
    "            nn.ReLU(),\n",
    "            #nn.BatchNorm1d(fc_units[1]),\n",
    "            nn.Linear(fc_units[1],action_size),\n",
    "            # nn.Tanh() # for continuous -1 to +1\n",
    "            nn.Softmax(dim=-1)\n",
    "            # nn.Sigmoid() # for 0-1\n",
    "        )\n",
    "        self.model.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self,m):\n",
    "        if (type(m) == nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            #nn.init.xavier_normal_(m.weight)\n",
    "            # nn.init.kaiming_normal_(m.weight)\n",
    "            m.bias.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        return self.model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size = state_size, action_size = action_size, seed=42, fc_units=FC_UNITS_CRITIC):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state, defaults to the global state size from the env\n",
    "            action_size (int): Dimension of each action, defaults to the global action size from the env\n",
    "            seed (int): Random seed\n",
    "            fc_units (list(int)): Number of nodes in the hidden layers as a list\n",
    "            ** Hard coded as a 3 layer network \n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.hc_1 = nn.Sequential(\n",
    "            nn.Linear(state_size,fc_units[0]),\n",
    "            nn.ReLU(), # leaky relu ?\n",
    "            # nn.BatchNorm1d(fc_units[0])\n",
    "        )\n",
    "        self.hc_2 = nn.Sequential(\n",
    "            nn.Linear(fc_units[0]+action_size,fc_units[1]),\n",
    "            nn.ReLU(), # leaky relu ?\n",
    "            nn.Linear(fc_units[1],1)\n",
    "        )\n",
    "        # Initialize the layers\n",
    "        self.hc_1.apply(self.init_weights)\n",
    "        self.hc_2.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = self.hc_1(state)\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = self.hc_2(x)\n",
    "        return (x)\n",
    "    \n",
    "    def init_weights(self,layer):\n",
    "        if (type(layer) == nn.Linear):\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            layer.bias.data.fill_(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed=42):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=False): #True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        #print(state)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return action # np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6. Instantiate an agent\n",
    "\n",
    "The state space and the action space dimensions come from the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=2, bias=True)\n",
      "    (5): Softmax()\n",
      "  )\n",
      ")\n",
      "Critic(\n",
      "  (hc_1): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hc_2): Sequential(\n",
      "    (0): Linear(in_features=18, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=42)\n",
    "print(agent.actor_local)\n",
    "print(agent.critic_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Learn & Train\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. DDPG Algorithm\n",
    "\n",
    "Define the DDPG Algorithm. Once we have defined the foundations (network, buffer, actor, critic, agent and so forth), the DDPG is relatively easy. It has a few responsibilities:\n",
    "1. Orchastrate the episodes calling the appropriate methods\n",
    "2. Display a running commentry of the scores and episode count\n",
    "3. Check the success criterion for solving the environment i.e. if running average is > 30 and print the episode count\n",
    "4. Store the model with the maximum score\n",
    "5. Keep track of the scores for analytics at the end of the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=1000):\n",
    "    scores_window = deque(maxlen=100)\n",
    "    scores = []\n",
    "    score  = 0\n",
    "    max_score = -np.Inf\n",
    "    has_seen_30 = False\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()                    # reset the environment    \n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        max_steps = 0\n",
    "        while True:\n",
    "            action = agent.act(state) # treat as softmax probabilities\n",
    "            act = int(np.random.choice(action_size, p=action)) # for Softmax\n",
    "            # act = 1 if action > 0.5 else 0 # for Sigmoid\n",
    "            next_state, reward, done, _ = env.step(act)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "            max_steps += 1\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:6.2f}\\tScore: {:6.2f}\\tMax_steps : {:3d}'.\\\n",
    "              format(i_episode, np.mean(scores_window), score, max_steps), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))  \n",
    "        if (np.mean(scores_window) >= 195.0) and (not has_seen_30):\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:5.2f}'.\\\n",
    "                  format(i_episode-100, np.mean(scores_window)))\n",
    "            # torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            has_seen_30 = True\n",
    "            break # Early stop\n",
    "            # To see how far it can go comment the break out\n",
    "        # Store the best model if desired\n",
    "        if STORE_MODELS:\n",
    "            if np.mean(scores_window) > max_score:\n",
    "                max_score = np.mean(scores_window)\n",
    "                torch.save(agent, 'checkpoint.pth')\n",
    "                # print(' .. Storing with score {}'.format(max_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. The actual training Run\n",
    "\n",
    "1. Run the DDPG\n",
    "2. Calculate and display end-of-run analytics viz. descriptive statistics and a plot of the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 9.61\tScore: 8.00\tMax_steps :   79\n",
      "Episode 200\tAverage Score: 71.03\tScore: 143.00\tMax_steps : 142\n",
      "Episode 300\tAverage Score: 114.96\tScore: 200.00\tMax_steps : 199\n",
      "Episode 382\tAverage Score: 195.17\tScore: 200.00\tMax_steps : 199\n",
      "Environment solved in 282 episodes!\tAverage Score: 195.17\n",
      "Elapsed : 0:02:29.415469\n",
      "2019-01-13 18:14:48.885969\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=2, bias=True)\n",
      "    (5): Softmax()\n",
      "  )\n",
      ")\n",
      "Max Score 200.000000 at 159\n",
      "Percentile [25,50,75] : [ 10.  82. 200.]\n",
      "Variance : 6435.419\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "scores = ddpg(n_episodes=1000) # 2000,1500 ; quick test 100,500\n",
    "env.close() # Close the environment\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())\n",
    "#\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "print(agent.actor_local)\n",
    "# print(agent.critic_local)\n",
    "print('Max Score {:.2f} at {}'.format(np.max(scores), np.argmax(scores)))\n",
    "print('Percentile [25,50,75] : {}'.format(np.percentile(scores,[25,50,75])))\n",
    "print('Variance : {:.3f}'.format(np.var(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Run logs & Notes\n",
    "\n",
    "1. A place to keep the statistics and qualitative observations\n",
    "2. It is easier to keep notes as soon as a run is done\n",
    "3. Also a place to keep future explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs\n",
    "#### 1/13/19\n",
    "\n",
    "1. Tried Softmax for actor, tried Sigmoid (with action_size=1) - was tanh()\n",
    "    * LR : Actor = 0.01, Critic = 0.001\n",
    "    * network 8 X 8\n",
    "1. Doesn't go beyond 10-11 steps when episode = 1500, buffer = 512\n",
    "2. No clue when it will wake up - episode = 15000; no dice so far\n",
    "3. Buffer = 2048 no change\n",
    "4. Took out batchnorm\n",
    "5. Back to softmax\n",
    "6. Took out clip(-1,1)\n",
    "    * No change score ~10\n",
    "7. tau = 0.05 - started learning - Am seeing the scores in the 20s! See if it passes nope stays at 20s\n",
    "8. network 400 X 300 ! goes back to a score of 10\n",
    "1. Network 36 X 8 ! Nope no good\n",
    "1. Network 4 X 4 - very small !\n",
    "1. Buffer = 4096, Batch = 32 meanders around a score of 10 for 10000 episodes !\n",
    "1. softmax(-1) definite progress - goes upto 200, but then gets 10 as well ! After 1100, it is steady !\n",
    " * Solved in 1029 episodes !\n",
    " * Was about to throw the towel ! Took me a day ! Was going to go back to A2C !\n",
    " * From 1300 onwards it gets prfect 200\n",
    " * __added noise and that saved the day !__\n",
    "1. back to 16 X 8 Network, batch = 64, 1500 episodes (was 15,000 ! and didn't do any good. The progress of a learning network is evident)\n",
    " * would have solved under 300, but a few black sheep epidodes bring the average down ;o(\n",
    " * 860 episodes. Between 300 and 800, it went down and came back !\n",
    "1. Cleaned up a little bit and ran again - 282 Episodes !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Run a stored Model or the learned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Here we are saving and loading the whole model. We cal also save & load the state dict\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from the saved file\n",
    "# The file has the parameters of the model that has the highest score during training\n",
    "# agent = torch.load('checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :  1 Score : 200.00 Steps : 199\n",
      "Episode :  2 Score : 200.00 Steps : 199\n",
      "Episode :  3 Score : 200.00 Steps : 199\n",
      "Episode :  4 Score : 200.00 Steps : 199\n",
      "Episode :  5 Score : 200.00 Steps : 199\n",
      "Episode :  6 Score : 200.00 Steps : 199\n",
      "Episode :  7 Score : 200.00 Steps : 199\n",
      "Episode :  8 Score : 200.00 Steps : 199\n",
      "Episode :  9 Score : 200.00 Steps : 199\n",
      "Episode : 10 Score : 200.00 Steps : 199\n",
      "Mean of 10 episodes = 200.0\n",
      "Elapsed : 0:00:00.517737\n",
      "2019-01-13 18:18:07.700678\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "scores=[]\n",
    "for i in range(10): # 10 episodes\n",
    "    state = env.reset()                                # reset the environment & get the current state\n",
    "    score = 0                                          # initialize the score\n",
    "    steps = 0                                          # Keep track of the number of steps\n",
    "    while True:\n",
    "        action = agent.act(state)                      # select an action, treat as softmax probabilities\n",
    "        act = int(np.random.choice(action_size, p=action)) # for Softmax\n",
    "        next_state, reward, done, _ = env.step(act)    # send the action to the environment\n",
    "        score += reward                                # update the score\n",
    "        state = next_state                             # roll over the state to next time step\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            break\n",
    "        else:\n",
    "            steps += 1\n",
    "    scores.append(score)\n",
    "    print(\"Episode : {:2d} Score : {:5.2f} Steps : {}\".format(i+1,score,steps))\n",
    "# Print stats at the end the run\n",
    "print('Mean of {} episodes = {}'.format(i+1,np.mean(scores)))\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
