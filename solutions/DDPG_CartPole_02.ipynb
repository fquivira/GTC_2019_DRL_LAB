{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On #11 : Balancing the Cart Pole w/ DDPG!\n",
    "---\n",
    "\n",
    "### Goal:\n",
    "- Implement DDPG on the CartPole Environment\n",
    "    * It is an overkill, but we will get a good understanding and also can compare against other algorithms\n",
    "    \n",
    "### Steps:\n",
    "1. Program DDPG Algorithm\n",
    "2. Run and Optimize\n",
    "3. Plot Values, like we did in other exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Organization\n",
    "#### The program has 3 parts :\n",
    "- Part 1 Defines the classes, initiates the environment and so forth. It sets up all the scaffolding needed\n",
    "- Part 2 Explore and Learn - it performs the DDPG Reinforcement Learning. It also saves the best model\n",
    "- Part 3 Run saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Definitions & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Install the required packages\n",
    "\n",
    "The required setup is detailed in the README.md\n",
    "\n",
    "I am running this on a MacBookPro 14,3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define imports\n",
    "\n",
    "python 3, numpy, matplotlib, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import copy\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants Definitions\n",
    "BUFFER_SIZE = 4096 # 2048 # 512 # int(1e5) # int(1e6) # int(1e5)  # replay buffer size  ?\n",
    "BATCH_SIZE = 32 # 64 # 32 # 128 # 64 # 256        # minibatch size for training\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 0.05 # 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 0.01 # 5e-4 # 1e-4 # 0.001 # 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 0.001 # 6e-4 # 3e-4  # 3e-3 # 0.001 # 3e-4        # learning rate of the critic 0.001\n",
    "WEIGHT_DECAY = 0.0001   # L2 weight decay\n",
    "# Number of neurons in the layers of the Actor & Critic Networks\n",
    "FC_UNITS_ACTOR = [16,8] # [4,4] # [32,16] #[400,300] #[8,8] #[128,128] # [64,128] # [32,16] # [400,300] # [128,128]\n",
    "FC_UNITS_CRITIC = [16,8]# [4,4] # [32,16] #[400,300] #[8,8] #[128,128] # [64,128] # [32,16] # [400,300] # [128,128]\n",
    "# Store models flag. Store during calibration runs and do not store during hyperparameter search\n",
    "# Used in Part 3 to run a stored model\n",
    "STORE_MODELS = False # True - Turn it on when you are ready to do the calibration training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import gym, PIL\\nenv = gym.make('CartPole-v0')\\narray = env.reset()\\nPIL.Image.fromarray(env.render(mode='rgb_array'))\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import gym, PIL\n",
    "env = gym.make('CartPole-v0')\n",
    "array = env.reset()\n",
    "PIL.Image.fromarray(env.render(mode='rgb_array'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Create instance & Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01258566, -0.00156614,  0.04207708, -0.00180545])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(42)\n",
    "# array = env.reset()\n",
    "env.reset()\n",
    "# env.render()\n",
    "#PIL.Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Examine the State and Action Spaces\n",
    "\n",
    "* The state space is continuous, with an observation space of 4 \n",
    "    * {x,$\\dot{x}$,$\\theta$, theta_dot}\n",
    "        * Cart Position,  Cart Velocity, Pole Angle, Pole Velocity at tip\n",
    "        * The angle, probably, is in radians\n",
    "\n",
    "The action space, on the contrary is simple viz. 0 = Left, 1 = Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n",
      "[0, 1]\n",
      "[ 0 = Left, 1 = Right ]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "act_space = [i for i in range(0,env.action_space.n)]\n",
    "print(act_space)\n",
    "# env.unwrapped.get_action_meanings() # AttributeError: 'FrozenLakeEnv' object has no attribute 'get_action_meanings'\n",
    "print('[ 0 = Left, 1 = Right ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_elapsed_seconds', '_elapsed_steps', '_episode_started_at', '_max_episode_seconds', '_max_episode_steps', '_past_limit', 'action_space', 'class_name', 'close', 'compute_reward', 'env', 'metadata', 'observation_space', 'render', 'reset', 'reward_range', 'seed', 'spec', 'step', 'unwrapped']\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'action_space', 'close', 'force_mag', 'gravity', 'kinematics_integrator', 'length', 'masscart', 'masspole', 'metadata', 'np_random', 'observation_space', 'polemass_length', 'render', 'reset', 'reward_range', 'seed', 'spec', 'state', 'step', 'steps_beyond_done', 'tau', 'theta_threshold_radians', 'total_mass', 'unwrapped', 'viewer', 'x_threshold']\n",
      "States =  Box(4,)\n",
      "Actions =  Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(dir(env))\n",
    "print(dir(env.unwrapped))\n",
    "# To see what functions and variables are availabe\n",
    "print('States = ',env.unwrapped.observation_space)\n",
    "print('Actions = ',env.unwrapped.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test the environment with Random Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [ 0.00560942  0.01842265 -0.03590751 -0.0120678 ] ] -> 0  : [ [ 0.00597787 -0.17616644 -0.03614886  0.26907314] ] R= 1.0\n",
      "[ [ 0.00597787 -0.17616644 -0.03614886  0.26907314] ] -> 1  : [ [ 0.00245454  0.01945224 -0.0307674  -0.03478864] ] R= 1.0\n",
      "[ [ 0.00245454  0.01945224 -0.0307674  -0.03478864] ] -> 1  : [ [ 0.00284359  0.21500159 -0.03146317 -0.33701802] ] R= 1.0\n",
      "[ [ 0.00284359  0.21500159 -0.03146317 -0.33701802] ] -> 0  : [ [ 0.00714362  0.02034118 -0.03820353 -0.05442056] ] R= 1.0\n",
      "[ [ 0.00714362  0.02034118 -0.03820353 -0.05442056] ] -> 1  : [ [ 0.00755044  0.2159895  -0.03929194 -0.35890801] ] R= 1.0\n",
      "[ [ 0.00755044  0.2159895  -0.03929194 -0.35890801] ] -> 1  : [ [ 0.01187023  0.41164734 -0.0464701  -0.66371712] ] R= 1.0\n",
      "[ [ 0.01187023  0.41164734 -0.0464701  -0.66371712] ] -> 1  : [ [ 0.02010318  0.60738391 -0.05974445 -0.97066231] ] R= 1.0\n",
      "[ [ 0.02010318  0.60738391 -0.05974445 -0.97066231] ] -> 1  : [ [ 0.03225086  0.80325467 -0.07915769 -1.28149866] ] R= 1.0\n",
      "[ [ 0.03225086  0.80325467 -0.07915769 -1.28149866] ] -> 1  : [ [ 0.04831595  0.99929074 -0.10478767 -1.59788005] ] R= 1.0\n",
      "[ [ 0.04831595  0.99929074 -0.10478767 -1.59788005] ] -> 1  : [ [ 0.06830176  1.19548692 -0.13674527 -1.92131128] ] R= 1.0\n",
      "[ [ 0.06830176  1.19548692 -0.13674527 -1.92131128] ] -> 1  : [ [ 0.0922115   1.39178778 -0.17517149 -2.25309178] ] R= 1.0\n",
      "[ [ 0.0922115   1.39178778 -0.17517149 -2.25309178] ] -> 0  : [ [ 0.12004726  1.19869297 -0.22023333 -2.0191195 ] ] R= 1.0\n",
      "Episode 1 finished after 12 steps with a Total Reward = 12\n",
      "[ [-0.03056663  0.02063487  0.01650017  0.04869769] ] -> 0  : [ [-0.03015393 -0.17471974  0.01747412  0.34654056] ] R= 1.0\n",
      "[ [-0.03015393 -0.17471974  0.01747412  0.34654056] ] -> 1  : [ [-0.03364832  0.02014935  0.02440493  0.05941868] ] R= 1.0\n",
      "[ [-0.03364832  0.02014935  0.02440493  0.05941868] ] -> 0  : [ [-0.03324534 -0.17531386  0.02559331  0.35970053] ] R= 1.0\n",
      "[ [-0.03324534 -0.17531386  0.02559331  0.35970053] ] -> 0  : [ [-0.03675161 -0.3707901   0.03278732  0.66034247] ] R= 1.0\n",
      "[ [-0.03675161 -0.3707901   0.03278732  0.66034247] ] -> 0  : [ [-0.04416742 -0.56635261  0.04599417  0.96316632] ] R= 1.0\n",
      "[ [-0.04416742 -0.56635261  0.04599417  0.96316632] ] -> 0  : [ [-0.05549447 -0.76206137  0.06525749  1.26993652] ] R= 1.0\n",
      "[ [-0.05549447 -0.76206137  0.06525749  1.26993652] ] -> 0  : [ [-0.0707357  -0.957953    0.09065622  1.58232061] ] R= 1.0\n",
      "[ [-0.0707357  -0.957953    0.09065622  1.58232061] ] -> 1  : [ [-0.08989476 -0.76401924  0.12230263  1.31923099] ] R= 1.0\n",
      "[ [-0.08989476 -0.76401924  0.12230263  1.31923099] ] -> 0  : [ [-0.10517514 -0.96045669  0.14868725  1.64755357] ] R= 1.0\n",
      "[ [-0.10517514 -0.96045669  0.14868725  1.64755357] ] -> 1  : [ [-0.12438427 -0.76735361  0.18163833  1.40464805] ] R= 1.0\n",
      "[ [-0.12438427 -0.76735361  0.18163833  1.40464805] ] -> 1  : [ [-0.13973135 -0.57489151  0.20973129  1.17381268] ] R= 1.0\n",
      "Episode 2 finished after 11 steps with a Total Reward = 11\n",
      "[ [-0.0313163   0.03775876 -0.01135978 -0.01095196] ] -> 0  : [ [-0.03056113 -0.15719845 -0.01157882  0.27812529] ] R= 1.0\n",
      "[ [-0.03056113 -0.15719845 -0.01157882  0.27812529] ] -> 0  : [ [-0.0337051  -0.35215332 -0.00601632  0.56713389] ] R= 1.0\n",
      "[ [-0.0337051  -0.35215332 -0.00601632  0.56713389] ] -> 1  : [ [-0.04074816 -0.15694749  0.00532636  0.27256166] ] R= 1.0\n",
      "[ [-0.04074816 -0.15694749  0.00532636  0.27256166] ] -> 1  : [ [-0.04388711  0.03809805  0.01077759 -0.01843656] ] R= 1.0\n",
      "[ [-0.04388711  0.03809805  0.01077759 -0.01843656] ] -> 1  : [ [-0.04312515  0.2330638   0.01040886 -0.30769964] ] R= 1.0\n",
      "[ [-0.04312515  0.2330638   0.01040886 -0.30769964] ] -> 1  : [ [-0.03846388  0.4280359   0.00425487 -0.5970818 ] ] R= 1.0\n",
      "[ [-0.03846388  0.4280359   0.00425487 -0.5970818 ] ] -> 0  : [ [-0.02990316  0.23285466 -0.00768677 -0.30306167] ] R= 1.0\n",
      "[ [-0.02990316  0.23285466 -0.00768677 -0.30306167] ] -> 1  : [ [-0.02524607  0.42808532 -0.013748   -0.59815889] ] R= 1.0\n",
      "[ [-0.02524607  0.42808532 -0.013748   -0.59815889] ] -> 0  : [ [-0.01668436  0.2331584  -0.02571118 -0.30983793] ] R= 1.0\n",
      "[ [-0.01668436  0.2331584  -0.02571118 -0.30983793] ] -> 1  : [ [-0.01202119  0.42863707 -0.03190794 -0.61051727] ] R= 1.0\n",
      "[ [-0.01202119  0.42863707 -0.03190794 -0.61051727] ] -> 0  : [ [-0.00344845  0.23397531 -0.04411828 -0.3280526 ] ] R= 1.0\n",
      "[ [-0.00344845  0.23397531 -0.04411828 -0.3280526 ] ] -> 1  : [ [ 0.00123106  0.42969667 -0.05067933 -0.63431554] ] R= 1.0\n",
      "[ [ 0.00123106  0.42969667 -0.05067933 -0.63431554] ] -> 1  : [ [ 0.00982499  0.62548754 -0.06336564 -0.94251813] ] R= 1.0\n",
      "[ [ 0.00982499  0.62548754 -0.06336564 -0.94251813] ] -> 0  : [ [ 0.02233474  0.43127402 -0.08221601 -0.67039954] ] R= 1.0\n",
      "[ [ 0.02233474  0.43127402 -0.08221601 -0.67039954] ] -> 1  : [ [ 0.03096022  0.62743695 -0.095624   -0.98779431] ] R= 1.0\n",
      "[ [ 0.03096022  0.62743695 -0.095624   -0.98779431] ] -> 1  : [ [ 0.04350896  0.82370014 -0.11537988 -1.30891479] ] R= 1.0\n",
      "[ [ 0.04350896  0.82370014 -0.11537988 -1.30891479] ] -> 0  : [ [ 0.05998296  0.63021347 -0.14155818 -1.05446096] ] R= 1.0\n",
      "[ [ 0.05998296  0.63021347 -0.14155818 -1.05446096] ] -> 0  : [ [ 0.07258723  0.4372227  -0.1626474  -0.80934968] ] R= 1.0\n",
      "[ [ 0.07258723  0.4372227  -0.1626474  -0.80934968] ] -> 1  : [ [ 0.08133169  0.634155   -0.17883439 -1.14845725] ] R= 1.0\n",
      "[ [ 0.08133169  0.634155   -0.17883439 -1.14845725] ] -> 0  : [ [ 0.09401479  0.44176002 -0.20180354 -0.91676484] ] R= 1.0\n",
      "[ [ 0.09401479  0.44176002 -0.20180354 -0.91676484] ] -> 1  : [ [ 0.10284999  0.63895416 -0.22013883 -1.26548183] ] R= 1.0\n",
      "Episode 3 finished after 21 steps with a Total Reward = 21\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(3):\n",
    "    state = env.reset()\n",
    "    tot_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        print('[',state,']','->', action,' : [',next_state,']', 'R=',reward)\n",
    "        # env.render()\n",
    "        tot_reward += reward\n",
    "        steps += 1\n",
    "        if done:\n",
    "            print('Episode {:d} finished after {:d} steps with a Total Reward = {:.0f}'.\n",
    "                  format(i_episode+1,steps, tot_reward))\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "# Pole angle +/-12 degrees, Cart Pos +/- 2.4 or 200 steps\n",
    "# Cart Pos, Velocity, Pole Angle, Velocity\n",
    "# 12 degrees = .2094 radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device = {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Algorithm\n",
    "\n",
    "We are using the DDPG. I liked the simple systems diagram that Prof. Sergey Levine from UC Berkeley uses for his CS294 Deep Reinforcement Learning class[http://rail.eecs.berkeley.edu/deeprlcourse/]\n",
    "<img src=\"RL_Systems_Flow.png\">\n",
    "\n",
    "The major components of the algorithm are:\n",
    "1. `Actor` implemented as a Deep Neural Network whih consists of fully connected layers.\n",
    "2. `Critic` implemented as a Deep Neural Network which consists of fully connected networks\n",
    "3. `Experience replay buffer` - in order to train the network we take actions and then store the results in the replay buffer. The replay buffer is a circular buffer and it has methods to sample a random batch\n",
    "3. `The Agent` brings all of the above together. It interacts with the environment by taking actions based on a policy, collects rewards and the observation feedback, then stores the experience in the replay buffer and also initiates a learning step on the actor and critic networks\n",
    "     * The agent has 3 main components viz:\n",
    "         1. The DDPG Orchestrator which interacts with the environment by taking actions and then unpacking the returned package to rewards, state space et al.\n",
    "         2. It also has to do housekeeping like tracking scores, store high performant models and check when the problem is solved\n",
    "         3. The 3rd component is the most interesting one - it gives the agent the capability to hunt for the right policy !!\n",
    "\n",
    "### Pseudo Code\n",
    "<img src=\"DDPG_Alg.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size=state_size, action_size=action_size, seed=42, fc_units=FC_UNITS_ACTOR):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state, defaults to the global state size from the env\n",
    "            action_size (int): Dimension of each action, defaults to the global action size from the env\n",
    "            seed (int): Random seed\n",
    "            fc_units (list(int)): Number of nodes in the hidden layers as a list\n",
    "            ** Hard coded as a 3 layer network \n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            #nn.BatchNorm1d(state_size),\n",
    "            nn.Linear(state_size,fc_units[0]),\n",
    "            nn.ReLU(),\n",
    "            #nn.BatchNorm1d(fc_units[0]),\n",
    "            nn.Linear(fc_units[0],fc_units[1]),\n",
    "            nn.ReLU(),\n",
    "            #nn.BatchNorm1d(fc_units[1]),\n",
    "            nn.Linear(fc_units[1],action_size),\n",
    "            # nn.Tanh() # for continuous -1 to +1\n",
    "            nn.Softmax(dim=-1)\n",
    "            # nn.Sigmoid() # for 0-1\n",
    "        )\n",
    "        self.model.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self,m):\n",
    "        if (type(m) == nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            #nn.init.xavier_normal_(m.weight)\n",
    "            # nn.init.kaiming_normal_(m.weight)\n",
    "            m.bias.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        return self.model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size = state_size, action_size = action_size, seed=42, fc_units=FC_UNITS_CRITIC):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state, defaults to the global state size from the env\n",
    "            action_size (int): Dimension of each action, defaults to the global action size from the env\n",
    "            seed (int): Random seed\n",
    "            fc_units (list(int)): Number of nodes in the hidden layers as a list\n",
    "            ** Hard coded as a 3 layer network \n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.hc_1 = nn.Sequential(\n",
    "            nn.Linear(state_size,fc_units[0]),\n",
    "            nn.ReLU(), # leaky relu ?\n",
    "            # nn.BatchNorm1d(fc_units[0])\n",
    "        )\n",
    "        self.hc_2 = nn.Sequential(\n",
    "            nn.Linear(fc_units[0]+action_size,fc_units[1]),\n",
    "            nn.ReLU(), # leaky relu ?\n",
    "            nn.Linear(fc_units[1],1)\n",
    "        )\n",
    "        # Initialize the layers\n",
    "        self.hc_1.apply(self.init_weights)\n",
    "        self.hc_2.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = self.hc_1(state)\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = self.hc_2(x)\n",
    "        return (x)\n",
    "    \n",
    "    def init_weights(self,layer):\n",
    "        if (type(layer) == nn.Linear):\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            layer.bias.data.fill_(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed=42):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=False): # True\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        #print(state)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return action # np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6. Instantiate an agent\n",
    "\n",
    "The state space and the action space dimensions come from the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=2, bias=True)\n",
      "    (5): Softmax()\n",
      "  )\n",
      ")\n",
      "Critic(\n",
      "  (hc_1): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (hc_2): Sequential(\n",
      "    (0): Linear(in_features=18, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=42)\n",
    "print(agent.actor_local)\n",
    "print(agent.critic_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Learn & Train\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. DDPG Algorithm\n",
    "\n",
    "Define the DDPG Algorithm. Once we have defined the foundations (network, buffer, actor, critic, agent and so forth), the DDPG is relatively easy. It has a few responsibilities:\n",
    "1. Orchastrate the episodes calling the appropriate methods\n",
    "2. Display a running commentry of the scores and episode count\n",
    "3. Check the success criterion for solving the environment i.e. if running average is > 195 and print the episode count\n",
    "4. Store the model with the maximum score\n",
    "5. Keep track of the scores for analytics at the end of the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=1000):\n",
    "    scores_window = deque(maxlen=100)\n",
    "    scores = []\n",
    "    score  = 0\n",
    "    max_score = -np.Inf\n",
    "    has_seen_195 = False\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()                    # reset the environment    \n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        max_steps = 0\n",
    "        while True:\n",
    "            action = agent.act(state) # treat as softmax probabilities\n",
    "            act = int(np.random.choice(action_size, p=action)) # for Softmax\n",
    "            # act = 1 if action > 0.5 else 0 # for Sigmoid\n",
    "            next_state, reward, done, _ = env.step(act)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "            max_steps += 1\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:6.2f}\\tScore: {:6.2f}\\tMax_steps : {:3d}'.\\\n",
    "              format(i_episode, np.mean(scores_window), score, max_steps), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))  \n",
    "        if (np.mean(scores_window) >= 195.0) and (not has_seen_195):\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:5.2f}'.\\\n",
    "                  format(i_episode-100, np.mean(scores_window)))\n",
    "            # torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            has_seen_195 = True\n",
    "            break # Early stop\n",
    "            # To see how far it can go :\n",
    "            # comment the break out. The has_seen_195 will stop printing the \"Environment Solved\" every time\n",
    "        # Store the best model if desired\n",
    "        if STORE_MODELS:\n",
    "            if np.mean(scores_window) > max_score:\n",
    "                max_score = np.mean(scores_window)\n",
    "                torch.save(agent, 'checkpoint.pth')\n",
    "                # print(' .. Storing with score {}'.format(max_score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. The actual training Run\n",
    "\n",
    "1. Run the DDPG\n",
    "2. Calculate and display end-of-run analytics viz. descriptive statistics and a plot of the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 9.5858\tScore:   8.00\tMax_steps :   7\n",
      "Episode 200\tAverage Score: 9.4444\tScore:  11.00\tMax_steps :  10\n",
      "Episode 300\tAverage Score: 9.3535\tScore:  10.00\tMax_steps :   9\n",
      "Episode 400\tAverage Score: 104.71\tScore: 129.00\tMax_steps : 128\n",
      "Episode 500\tAverage Score: 149.30\tScore: 170.00\tMax_steps : 169\n",
      "Episode 600\tAverage Score: 173.89\tScore: 200.00\tMax_steps : 199\n",
      "Episode 633\tAverage Score: 195.01\tScore: 200.00\tMax_steps : 199\n",
      "Environment solved in 533 episodes!\tAverage Score: 195.01\n",
      "Elapsed : 0:03:13.291465\n",
      "2019-02-02 09:44:52.322328\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=8, out_features=2, bias=True)\n",
      "    (5): Softmax()\n",
      "  )\n",
      ")\n",
      "Max Score 200.00 at 362\n",
      "Percentile [25,50,75] : [  9.  34. 167.]\n",
      "Variance : 6487.119\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "scores = ddpg(n_episodes=1500) # 2000,1500, 1000 ; For quick functional test use 100,500\n",
    "env.close() # Close the environment\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())\n",
    "#\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "print(agent.actor_local)\n",
    "# print(agent.critic_local)\n",
    "print('Max Score {:.2f} at {}'.format(np.max(scores), np.argmax(scores)))\n",
    "print('Percentile [25,50,75] : {}'.format(np.percentile(scores,[25,50,75])))\n",
    "print('Variance : {:.3f}'.format(np.var(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Run\n",
    "<img src=\"ddpg_run_01.png\">\n",
    "<img src=\"ddpg_run.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Run logs & Notes\n",
    "\n",
    "1. A place to keep the statistics and qualitative observations\n",
    "2. It is easier to keep notes as soon as a run is done\n",
    "3. Also a place to keep future explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs\n",
    "#### 1/13/19\n",
    "\n",
    "1. Tried Softmax for actor, tried Sigmoid (with action_size=1) - was tanh()\n",
    "    * LR : Actor = 0.01, Critic = 0.001\n",
    "    * network 8 X 8\n",
    "1. Doesn't go beyond 10-11 steps when episode = 1500, buffer = 512\n",
    "2. No clue when it will wake up - episode = 15000; no dice so far\n",
    "3. Buffer = 2048 no change\n",
    "4. Took out batchnorm\n",
    "5. Back to softmax\n",
    "6. Took out clip(-1,1)\n",
    "    * No change score ~10\n",
    "7. tau = 0.05 - started learning - Am seeing the scores in the 20s! See if it passes nope stays at 20s\n",
    "8. network 400 X 300 ! goes back to a score of 10\n",
    "1. Network 36 X 8 ! Nope no good\n",
    "1. Network 4 X 4 - very small !\n",
    "1. Buffer = 4096, Batch = 32 meanders around a score of 10 for 10000 episodes !\n",
    "1. softmax(-1) definite progress - goes upto 200, but then gets 10 as well ! After 1100, it is steady !\n",
    " * Solved in 1029 episodes !\n",
    " * Was about to throw the towel ! Took me a day ! Was going to go back to A2C !\n",
    " * From 1300 onwards it gets prfect 200\n",
    " * __added noise and that saved the day !__\n",
    "1. back to 16 X 8 Network, batch = 64, 1500 episodes (was 15,000 ! and didn't do any good. The progress of a learning network is evident)\n",
    " * would have solved under 300, but a few black sheep epidodes bring the average down ;o(\n",
    " * 860 episodes. Between 300 and 800, it went down and came back !\n",
    "1. Cleaned up a little bit and ran again - 282 Episodes !\n",
    "\n",
    "#### 2/2/19\n",
    "\n",
    "1. Again got into a rut-nothing beyond a score of 11\n",
    "2. Changd network to FC4-FC4\n",
    "3. Better but - couldn't solve with 1000 episodes, learns and then unlearns\n",
    "4. Increasing episodes back to 15000. Nope\n",
    "5. Batch = 32; good steady progress. Woould it make it ? Nope\n",
    "6. Noise = True, was False for some reason. won't work for softmax\n",
    "7. Network FC16-FC8 - solved in 533 Steps !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Run a stored Model or the learned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Here we are saving and loading the whole model. We cal also save & load the state dict\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from the saved file\n",
    "# The file has the parameters of the model that has the highest score during training\n",
    "# agent = torch.load('checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :  1 Score : 200.00 Steps : 199\n",
      "Episode :  2 Score : 200.00 Steps : 199\n",
      "Episode :  3 Score : 11.00 Steps : 10\n",
      "Episode :  4 Score : 200.00 Steps : 199\n",
      "Episode :  5 Score : 12.00 Steps : 11\n",
      "Episode :  6 Score : 11.00 Steps : 10\n",
      "Episode :  7 Score : 12.00 Steps : 11\n",
      "Episode :  8 Score : 200.00 Steps : 199\n",
      "Episode :  9 Score : 200.00 Steps : 199\n",
      "Episode : 10 Score : 11.00 Steps : 10\n",
      "Mean of 10 episodes = 105.7\n",
      "Elapsed : 0:00:00.280475\n",
      "2019-02-02 09:47:52.658381\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "scores=[]\n",
    "for i in range(10): # 10 episodes\n",
    "    state = env.reset()                                # reset the environment & get the current state\n",
    "    score = 0                                          # initialize the score\n",
    "    steps = 0                                          # Keep track of the number of steps\n",
    "    while True:\n",
    "        action = agent.act(state)                      # select an action, treat as softmax probabilities\n",
    "        act = int(np.random.choice(action_size, p=action)) # for Softmax\n",
    "        next_state, reward, done, _ = env.step(act)    # send the action to the environment\n",
    "        score += reward                                # update the score\n",
    "        state = next_state                             # roll over the state to next time step\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            break\n",
    "        else:\n",
    "            steps += 1\n",
    "    scores.append(score)\n",
    "    print(\"Episode : {:2d} Score : {:5.2f} Steps : {}\".format(i+1,score,steps))\n",
    "# Print stats at the end the run\n",
    "print('Mean of {} episodes = {}'.format(i+1,np.mean(scores)))\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _That's All Folks !!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
