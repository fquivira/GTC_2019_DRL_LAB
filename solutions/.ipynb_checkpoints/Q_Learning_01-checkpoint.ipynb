{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On #4 - Balancing Cart Pole w/Q-learning\n",
    "\n",
    "## Goal:\n",
    "* Introduce the CartPole Environment\n",
    "    * More complex, Continuous\n",
    "* Implement Q-Learning for digitized CartPole\n",
    "    * Later we will use function approx and remove the requirement for digitization\n",
    "\n",
    "## Steps:\n",
    "1. Get familiar with Cartpole environment\n",
    "    * np.linspace() and np.digitize() for state space aggregation\n",
    "2. Program Q Learning\n",
    "3. Track & Plot Metrics to solve Cart Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : \n",
    "* Based on Udacity github https://github.com/udacity/deep-reinforcement-learning/tree/master/monte-carlo plus\n",
    "* My solution for the DQN https://github.com/xsankar/DQN_Navigation/blob/master/Navigation-v2.ipynb\n",
    "* Kaggle https://www.kaggle.com/sandovaledwin/q-learning-algorithm-for-solving-frozenlake-game/notebook\n",
    "* Phil Tabor's RL in Motion github https://github.com/philtabor/Reinforcement-Learning-In-Motion/tree/master/Unit-7-The-Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install the required packages\n",
    "\n",
    "* No esoteric requirements\n",
    "* You can run them without docker\n",
    "* pip install -r requirements.txt\n",
    "* Requirements\n",
    " * python 3.6, pytorch, openAI gym, numpy, matplotlib\n",
    " * anaconda is easier but not needed\n",
    " * Miniconda works fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define imports\n",
    "\n",
    "python 3, numpy, matplotlib, torch, gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import gym\n",
    "import PIL # for in-line display of certain environments\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Global Constants and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants Definitions\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "# Number of neurons in the layers of the Q Network\n",
    "FC1_UNITS = 16\n",
    "FC2_UNITS = 8\n",
    "FC3_UNITS = 4\n",
    "# Store models flag. Store during calibration runs and do not store during hyperparameter search\n",
    "STORE_MODELS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20943951023931956\n",
      "-0.20943951023931956\n"
     ]
    }
   ],
   "source": [
    "# Work area to quickly test utility functions\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "'''\n",
    "start_time = time.time()\n",
    "time.sleep(10)\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "'''\n",
    "print(math.radians(12))\n",
    "print(math.radians(-12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Create instance & Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02948199, 0.02003764, 0.04221512, 0.02956406])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym, PIL\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "# array = env.reset()\n",
    "# ** render doesn't work reliably on a server. Uncomment when running ** locally **\n",
    "# env.render()\n",
    "# PIL.Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This what it will look like\n",
    "### We don't need the render(). We run it on headless mode and inspect the results\n",
    "<img src=\"CartPole_Render.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Examine the State and Action Spaces\n",
    "\n",
    "* The state space is continuous, with an observation space of 4 \n",
    "    * {x,$\\dot{x}$,$\\theta$, theta_dot}\n",
    "        * Cart Position,  Cart Velocity, Pole Angle, Pole Velocity at tip\n",
    "        * The angle, probably, is in radians\n",
    "\n",
    "The action space, on the contrary is simple viz. 0 = Left, 1 = Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n",
      "[0, 1]\n",
      "[ 0 = Left, 1 = Right ]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "act_space = [i for i in range(0,env.action_space.n)]\n",
    "print(act_space)\n",
    "# env.unwrapped.get_action_meanings() # AttributeError: 'FrozenLakeEnv' object has no attribute 'get_action_meanings'\n",
    "print('[ 0 = Left, 1 = Right ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_elapsed_seconds', '_elapsed_steps', '_episode_started_at', '_max_episode_seconds', '_max_episode_steps', '_past_limit', 'action_space', 'class_name', 'close', 'compute_reward', 'env', 'metadata', 'observation_space', 'render', 'reset', 'reward_range', 'seed', 'spec', 'step', 'unwrapped']\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'action_space', 'close', 'force_mag', 'gravity', 'kinematics_integrator', 'length', 'masscart', 'masspole', 'metadata', 'np_random', 'observation_space', 'polemass_length', 'render', 'reset', 'reward_range', 'seed', 'spec', 'state', 'step', 'steps_beyond_done', 'tau', 'theta_threshold_radians', 'total_mass', 'unwrapped', 'viewer', 'x_threshold']\n",
      "States =  Box(4,)\n",
      "Actions =  Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(dir(env))\n",
    "print(dir(env.unwrapped))\n",
    "print('States = ',env.unwrapped.observation_space)\n",
    "print('Actions = ',env.unwrapped.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test the environment with Random Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [-0.04283321  0.01461015  0.03693223 -0.0010063 ] ] -> 1  : [ [-0.04254101  0.20918351  0.03691211 -0.28181171] ] R= 1.0\n",
      "[ [-0.04254101  0.20918351  0.03691211 -0.28181171] ] -> 1  : [ [-0.03835734  0.40376004  0.03127587 -0.56262799] ] R= 1.0\n",
      "[ [-0.03835734  0.40376004  0.03127587 -0.56262799] ] -> 0  : [ [-0.03028214  0.20821349  0.02002331 -0.260258  ] ] R= 1.0\n",
      "[ [-0.03028214  0.20821349  0.02002331 -0.260258  ] ] -> 1  : [ [-0.02611787  0.40304396  0.01481815 -0.54655867] ] R= 1.0\n",
      "[ [-0.02611787  0.40304396  0.01481815 -0.54655867] ] -> 0  : [ [-0.01805699  0.20771698  0.00388698 -0.24924399] ] R= 1.0\n",
      "[ [-0.01805699  0.20771698  0.00388698 -0.24924399] ] -> 1  : [ [-0.01390265  0.40278321 -0.0010979  -0.54069835] ] R= 1.0\n",
      "[ [-0.01390265  0.40278321 -0.0010979  -0.54069835] ] -> 1  : [ [-0.00584698  0.59792058 -0.01191187 -0.83372701] ] R= 1.0\n",
      "[ [-0.00584698  0.59792058 -0.01191187 -0.83372701] ] -> 1  : [ [ 0.00611143  0.79320324 -0.02858641 -1.13013223] ] R= 1.0\n",
      "[ [ 0.00611143  0.79320324 -0.02858641 -1.13013223] ] -> 1  : [ [ 0.02197549  0.98868764 -0.05118905 -1.43164228] ] R= 1.0\n",
      "[ [ 0.02197549  0.98868764 -0.05118905 -1.43164228] ] -> 0  : [ [ 0.04174925  0.79423347 -0.0798219  -1.1553861 ] ] R= 1.0\n",
      "[ [ 0.04174925  0.79423347 -0.0798219  -1.1553861 ] ] -> 1  : [ [ 0.05763391  0.99030026 -0.10292962 -1.47199258] ] R= 1.0\n",
      "[ [ 0.05763391  0.99030026 -0.10292962 -1.47199258] ] -> 0  : [ [ 0.07743992  0.79657652 -0.13236947 -1.21315281] ] R= 1.0\n",
      "[ [ 0.07743992  0.79657652 -0.13236947 -1.21315281] ] -> 0  : [ [ 0.09337145  0.60338757 -0.15663253 -0.96470751] ] R= 1.0\n",
      "[ [ 0.09337145  0.60338757 -0.15663253 -0.96470751] ] -> 1  : [ [ 0.1054392   0.80022719 -0.17592668 -1.30221433] ] R= 1.0\n",
      "[ [ 0.1054392   0.80022719 -0.17592668 -1.30221433] ] -> 1  : [ [ 0.12144375  0.99708965 -0.20197096 -1.64440615] ] R= 1.0\n",
      "[ [ 0.12144375  0.99708965 -0.20197096 -1.64440615] ] -> 0  : [ [ 0.14138554  0.80482304 -0.23485909 -1.42084508] ] R= 1.0\n",
      "Episode 1 finished after 16 steps with a Total Reward = 16\n",
      "[ [ 0.0031416  -0.02864543  0.02726326  0.01631114] ] -> 1  : [ [ 0.00256869  0.16607514  0.02758948 -0.26764677] ] R= 1.0\n",
      "[ [ 0.00256869  0.16607514  0.02758948 -0.26764677] ] -> 0  : [ [ 0.0058902  -0.02942947  0.02223654  0.03360882] ] R= 1.0\n",
      "[ [ 0.0058902  -0.02942947  0.02223654  0.03360882] ] -> 1  : [ [ 0.00530161  0.16536666  0.02290872 -0.25197613] ] R= 1.0\n",
      "[ [ 0.00530161  0.16536666  0.02290872 -0.25197613] ] -> 0  : [ [ 0.00860894 -0.0300748   0.0178692   0.0478437 ] ] R= 1.0\n",
      "[ [ 0.00860894 -0.0300748   0.0178692   0.0478437 ] ] -> 0  : [ [ 0.00800744 -0.22544837  0.01882607  0.34611053] ] R= 1.0\n",
      "[ [ 0.00800744 -0.22544837  0.01882607  0.34611053] ] -> 0  : [ [ 0.00349848 -0.42083298  0.02574828  0.64467005] ] R= 1.0\n",
      "[ [ 0.00349848 -0.42083298  0.02574828  0.64467005] ] -> 0  : [ [-0.00491818 -0.61630412  0.03864168  0.94534873] ] R= 1.0\n",
      "[ [-0.00491818 -0.61630412  0.03864168  0.94534873] ] -> 0  : [ [-0.01724427 -0.81192465  0.05754866  1.24991831] ] R= 1.0\n",
      "[ [-0.01724427 -0.81192465  0.05754866  1.24991831] ] -> 1  : [ [-0.03348276 -0.61758559  0.08254702  0.97580226] ] R= 1.0\n",
      "[ [-0.03348276 -0.61758559  0.08254702  0.97580226] ] -> 1  : [ [-0.04583447 -0.42366198  0.10206307  0.71014861] ] R= 1.0\n",
      "[ [-0.04583447 -0.42366198  0.10206307  0.71014861] ] -> 0  : [ [-0.05430771 -0.62003823  0.11626604  1.03313458] ] R= 1.0\n",
      "[ [-0.05430771 -0.62003823  0.11626604  1.03313458] ] -> 0  : [ [-0.06670848 -0.81649838  0.13692873  1.35994051] ] R= 1.0\n",
      "[ [-0.06670848 -0.81649838  0.13692873  1.35994051] ] -> 0  : [ [-0.08303844 -1.01304537  0.16412754  1.69213283] ] R= 1.0\n",
      "[ [-0.08303844 -1.01304537  0.16412754  1.69213283] ] -> 1  : [ [-0.10329935 -0.82015557  0.1979702   1.45472356] ] R= 1.0\n",
      "[ [-0.10329935 -0.82015557  0.1979702   1.45472356] ] -> 1  : [ [-0.11970246 -0.6279372   0.22706467  1.22985146] ] R= 1.0\n",
      "Episode 2 finished after 15 steps with a Total Reward = 15\n",
      "[ [-0.03661675  0.00948912 -0.01539317  0.01332777] ] -> 0  : [ [-0.03642696 -0.18540873 -0.01512661  0.30111449] ] R= 1.0\n",
      "[ [-0.03642696 -0.18540873 -0.01512661  0.30111449] ] -> 1  : [ [-0.04013514  0.00992552 -0.00910432  0.00369959] ] R= 1.0\n",
      "[ [-0.04013514  0.00992552 -0.00910432  0.00369959] ] -> 0  : [ [-0.03993663 -0.18506469 -0.00903033  0.29349614] ] R= 1.0\n",
      "[ [-0.03993663 -0.18506469 -0.00903033  0.29349614] ] -> 0  : [ [-0.04363792 -0.38005674 -0.00316041  0.5833174 ] ] R= 1.0\n",
      "[ [-0.04363792 -0.38005674 -0.00316041  0.5833174 ] ] -> 1  : [ [-0.05123906 -0.18489065  0.00850594  0.28964058] ] R= 1.0\n",
      "[ [-0.05123906 -0.18489065  0.00850594  0.28964058] ] -> 0  : [ [-0.05493687 -0.38013286  0.01429875  0.584994  ] ] R= 1.0\n",
      "[ [-0.05493687 -0.38013286  0.01429875  0.584994  ] ] -> 1  : [ [-0.06253953 -0.18521408  0.02599863  0.29684942] ] R= 1.0\n",
      "[ [-0.06253953 -0.18521408  0.02599863  0.29684942] ] -> 1  : [ [-0.06624381  0.00952778  0.03193562  0.01247808] ] R= 1.0\n",
      "[ [-0.06624381  0.00952778  0.03193562  0.01247808] ] -> 1  : [ [-0.06605325  0.20417753  0.03218518 -0.26996018] ] R= 1.0\n",
      "[ [-0.06605325  0.20417753  0.03218518 -0.26996018] ] -> 1  : [ [-0.0619697   0.39882575  0.02678598 -0.5523205 ] ] R= 1.0\n",
      "[ [-0.0619697   0.39882575  0.02678598 -0.5523205 ] ] -> 1  : [ [-0.05399319  0.59356149  0.01573957 -0.83644519] ] R= 1.0\n",
      "[ [-0.05399319  0.59356149  0.01573957 -0.83644519] ] -> 1  : [ [-4.21219574e-02  7.88464958e-01 -9.89338056e-04 -1.12413694e+00] ] R= 1.0\n",
      "[ [-4.21219574e-02  7.88464958e-01 -9.89338056e-04 -1.12413694e+00] ] -> 0  : [ [-0.02635266  0.59335599 -0.02347208 -0.83176449] ] R= 1.0\n",
      "[ [-0.02635266  0.59335599 -0.02347208 -0.83176449] ] -> 1  : [ [-0.01448554  0.78879072 -0.04010737 -1.131736  ] ] R= 1.0\n",
      "[ [-0.01448554  0.78879072 -0.04010737 -1.131736  ] ] -> 1  : [ [ 1.29027601e-03  9.84414139e-01 -6.27420867e-02 -1.43672355e+00] ] R= 1.0\n",
      "[ [ 1.29027601e-03  9.84414139e-01 -6.27420867e-02 -1.43672355e+00] ] -> 0  : [ [ 0.02097856  0.79011926 -0.09147656 -1.16428876] ] R= 1.0\n",
      "[ [ 0.02097856  0.79011926 -0.09147656 -1.16428876] ] -> 0  : [ [ 0.03678094  0.59629945 -0.11476233 -0.90163121] ] R= 1.0\n",
      "[ [ 0.03678094  0.59629945 -0.11476233 -0.90163121] ] -> 1  : [ [ 0.04870693  0.79277352 -0.13279496 -1.22806983] ] R= 1.0\n",
      "[ [ 0.04870693  0.79277352 -0.13279496 -1.22806983] ] -> 0  : [ [ 0.0645624   0.59958672 -0.15735635 -0.97976802] ] R= 1.0\n",
      "[ [ 0.0645624   0.59958672 -0.15735635 -0.97976802] ] -> 0  : [ [ 0.07655414  0.40688393 -0.17695171 -0.74035717] ] R= 1.0\n",
      "[ [ 0.07655414  0.40688393 -0.17695171 -0.74035717] ] -> 1  : [ [ 0.08469182  0.60395004 -0.19175886 -1.08309325] ] R= 1.0\n",
      "[ [ 0.08469182  0.60395004 -0.19175886 -1.08309325] ] -> 1  : [ [ 0.09677082  0.8010133  -0.21342072 -1.42930227] ] R= 1.0\n",
      "Episode 3 finished after 22 steps with a Total Reward = 22\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(3):\n",
    "    state = env.reset()\n",
    "    tot_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        print('[',state,']','->', action,' : [',next_state,']', 'R=',reward)\n",
    "        # env.render()\n",
    "        tot_reward += reward\n",
    "        steps += 1\n",
    "        if done:\n",
    "            print('Episode {:d} finished after {:d} steps with a Total Reward = {:.0f}'.\n",
    "                  format(i_episode+1,steps, tot_reward))\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "# Pole angle +/-12 degrees, Cart Pos +/- 2.4 or 200 steps\n",
    "# Cart Pos, Velocity, Pole Angle, Velocity\n",
    "# 12 degrees = .2094 radians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning\n",
    "### Let us implement the Basic Q-Learning Algorithm\n",
    "<img src='../Qlearning_Alg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Define policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-Greedy\n",
    "\n",
    "<img src=\"../e_greedy.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_e_greedy_action(env,Q,state,epsilon,nA):\n",
    "    action = np.random.choice(np.arange(nA), p=get_probs(Q[state], epsilon, nA)) if state in Q else env.action_space.sample()\n",
    "    return action\n",
    "\n",
    "def get_probs(Q_s, epsilon, nA):\n",
    "    \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \"\"\"\n",
    "    policy_s = np.ones(nA) * epsilon / nA\n",
    "    best_a = np.argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "    return policy_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to convert the sontinuous state space to discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discretize the spaces\n",
    "poleThetaSpace = np.linspace(-0.20943951, 0.20943951, 10)\n",
    "poleThetaVelSpace = np.linspace(-4, 4, 10)\n",
    "cartPosSpace = np.linspace(-2.4, 2.4, 10)\n",
    "cartVelSpace = np.linspace(-4, 4, 10)\n",
    "\n",
    "def getState(observation):\n",
    "    cartX, cartXdot, cartTheta, cartThetadot = observation\n",
    "    cartX = int(np.digitize(cartX, cartPosSpace))\n",
    "    cartXdot = int(np.digitize(cartXdot, cartVelSpace))\n",
    "    cartTheta = int(np.digitize(cartTheta, poleThetaSpace))\n",
    "    cartThetadot = int(np.digitize(cartThetadot, poleThetaVelSpace))\n",
    "\n",
    "    return (cartX, cartXdot, cartTheta, cartThetadot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses global variavle env\n",
    "def q_learning(n_episodes=2000, max_t=1000, \n",
    "               epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.9995, # 0.995 gets to min by 1000 episodes\n",
    "               alpha=0.01, gamma=1.0):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        epsilon_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        epsilon_end (float): minimum value of epsilon\n",
    "        epsilon_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        alpha = step-size parameter\n",
    "        gamma = discount rate\n",
    "    \"\"\"\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(num_actions))\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    epsilon = epsilon_start            # initialize epsilon\n",
    "    has_seen_13 = False\n",
    "    max_score = 0\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        score = 0\n",
    "        max_steps = 0\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, n_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        s_t = env.reset()\n",
    "        a_t = choose_e_greedy_action(env,Q,getState(s_t),epsilon,num_actions)\n",
    "        t = 0\n",
    "        while True:\n",
    "            s_t_d = getState(s_t)\n",
    "            # state, reward, done, info = env.step(action)\n",
    "            s_t_1, reward, done, prob = env.step(a_t)\n",
    "            # print(state,reward,done, prob)\n",
    "            s_t_1_d = getState(s_t_1) # Digitize state\n",
    "            a_t_1 = choose_e_greedy_action(env,Q,s_t_1_d,epsilon,num_actions)\n",
    "            best_a = np.argmax(Q[s_t_1_d])\n",
    "            Q[s_t_d][a_t] = Q[s_t_d][a_t] + alpha * (reward + gamma*(Q[s_t_1_d][best_a]) - Q[s_t_d][a_t])\n",
    "            a_t = a_t_1\n",
    "            s_t = s_t_1\n",
    "            score += reward\n",
    "            max_steps += 1\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        epsilon = max(epsilon*epsilon_decay, epsilon_min) # decrease epsilon\n",
    "        print('\\rEpisode : {}\\tAverage Score : {:5.2f}\\tMax_steps : {}\\teps : {:5.3f}\\tMax.Score : {:5.3f}'.\\\n",
    "              format(i_episode, np.mean(scores_window),max_steps,epsilon,max_score), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode : {}\\tAverage Score : {:5.2f}\\tMax_steps : {}\\teps : {:5.3f}\\tMax.Score : {:5.3f}'.\\\n",
    "                  format(i_episode, np.mean(scores_window),max_steps,epsilon,max_score))\n",
    "        if (np.mean(scores_window)>=195.0) and (not has_seen_13):\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:5.2f}'.\\\n",
    "                  format(i_episode-100, np.mean(scores_window)))\n",
    "            # torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            has_seen_13 = True\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "    return scores, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 100\tAverage Score : 23.28\tMax_steps : 22\teps : 0.951\tMax.Score : 76.000\n",
      "Episode : 200\tAverage Score : 26.34\tMax_steps : 23\teps : 0.905\tMax.Score : 98.000\n",
      "Episode : 300\tAverage Score : 25.10\tMax_steps : 16\teps : 0.861\tMax.Score : 98.000\n",
      "Episode : 400\tAverage Score : 24.75\tMax_steps : 16\teps : 0.819\tMax.Score : 98.000\n",
      "Episode : 500\tAverage Score : 24.74\tMax_steps : 29\teps : 0.779\tMax.Score : 98.000\n",
      "Episode : 600\tAverage Score : 22.63\tMax_steps : 14\teps : 0.741\tMax.Score : 98.000\n",
      "Episode : 700\tAverage Score : 26.43\tMax_steps : 25\teps : 0.705\tMax.Score : 98.000\n",
      "Episode : 800\tAverage Score : 23.04\tMax_steps : 27\teps : 0.670\tMax.Score : 98.000\n",
      "Episode : 900\tAverage Score : 25.26\tMax_steps : 21\teps : 0.638\tMax.Score : 121.000\n",
      "Episode : 1000\tAverage Score : 26.06\tMax_steps : 19\teps : 0.606\tMax.Score : 121.000\n",
      "Episode : 1100\tAverage Score : 26.75\tMax_steps : 21\teps : 0.577\tMax.Score : 121.000\n",
      "Episode : 1200\tAverage Score : 25.79\tMax_steps : 28\teps : 0.549\tMax.Score : 121.000\n",
      "Episode : 1300\tAverage Score : 25.73\tMax_steps : 23\teps : 0.522\tMax.Score : 121.000\n",
      "Episode : 1400\tAverage Score : 24.51\tMax_steps : 24\teps : 0.496\tMax.Score : 121.000\n",
      "Episode : 1500\tAverage Score : 25.04\tMax_steps : 26\teps : 0.472\tMax.Score : 121.000\n",
      "Episode : 1600\tAverage Score : 24.18\tMax_steps : 21\teps : 0.449\tMax.Score : 121.000\n",
      "Episode : 1700\tAverage Score : 24.97\tMax_steps : 32\teps : 0.427\tMax.Score : 121.000\n",
      "Episode : 1800\tAverage Score : 25.21\tMax_steps : 33\teps : 0.406\tMax.Score : 121.000\n",
      "Episode : 1900\tAverage Score : 24.60\tMax_steps : 24\teps : 0.387\tMax.Score : 121.000\n",
      "Episode : 2000\tAverage Score : 24.58\tMax_steps : 19\teps : 0.368\tMax.Score : 121.000\n",
      "Episode : 2100\tAverage Score : 23.43\tMax_steps : 13\teps : 0.350\tMax.Score : 121.000\n",
      "Episode : 2200\tAverage Score : 24.62\tMax_steps : 21\teps : 0.333\tMax.Score : 121.000\n",
      "Episode : 2300\tAverage Score : 24.64\tMax_steps : 29\teps : 0.317\tMax.Score : 121.000\n",
      "Episode : 2400\tAverage Score : 23.73\tMax_steps : 14\teps : 0.301\tMax.Score : 121.000\n",
      "Episode : 2500\tAverage Score : 24.40\tMax_steps : 30\teps : 0.286\tMax.Score : 121.000\n",
      "Episode : 2600\tAverage Score : 28.87\tMax_steps : 40\teps : 0.272\tMax.Score : 121.000\n",
      "Episode : 2700\tAverage Score : 30.22\tMax_steps : 37\teps : 0.259\tMax.Score : 121.000\n",
      "Episode : 2800\tAverage Score : 30.47\tMax_steps : 37\teps : 0.247\tMax.Score : 121.000\n",
      "Episode : 2900\tAverage Score : 29.23\tMax_steps : 20\teps : 0.234\tMax.Score : 121.000\n",
      "Episode : 3000\tAverage Score : 30.23\tMax_steps : 21\teps : 0.223\tMax.Score : 121.000\n",
      "Episode : 3100\tAverage Score : 30.29\tMax_steps : 26\teps : 0.212\tMax.Score : 121.000\n",
      "Episode : 3200\tAverage Score : 30.97\tMax_steps : 23\teps : 0.202\tMax.Score : 121.000\n",
      "Episode : 3300\tAverage Score : 29.34\tMax_steps : 30\teps : 0.192\tMax.Score : 121.000\n",
      "Episode : 3400\tAverage Score : 30.12\tMax_steps : 32\teps : 0.183\tMax.Score : 121.000\n",
      "Episode : 3500\tAverage Score : 28.89\tMax_steps : 19\teps : 0.174\tMax.Score : 121.000\n",
      "Episode : 3600\tAverage Score : 30.39\tMax_steps : 37\teps : 0.165\tMax.Score : 121.000\n",
      "Episode : 3700\tAverage Score : 30.78\tMax_steps : 37\teps : 0.157\tMax.Score : 121.000\n",
      "Episode : 3800\tAverage Score : 30.17\tMax_steps : 35\teps : 0.149\tMax.Score : 121.000\n",
      "Episode : 3900\tAverage Score : 30.00\tMax_steps : 28\teps : 0.142\tMax.Score : 121.000\n",
      "Episode : 4000\tAverage Score : 30.71\tMax_steps : 33\teps : 0.135\tMax.Score : 121.000\n",
      "Episode : 4100\tAverage Score : 31.06\tMax_steps : 37\teps : 0.129\tMax.Score : 121.000\n",
      "Episode : 4200\tAverage Score : 30.25\tMax_steps : 29\teps : 0.122\tMax.Score : 121.000\n",
      "Episode : 4300\tAverage Score : 31.13\tMax_steps : 28\teps : 0.116\tMax.Score : 121.000\n",
      "Episode : 4400\tAverage Score : 31.25\tMax_steps : 33\teps : 0.111\tMax.Score : 121.000\n",
      "Episode : 4500\tAverage Score : 30.97\tMax_steps : 34\teps : 0.105\tMax.Score : 121.000\n",
      "Episode : 4600\tAverage Score : 31.38\tMax_steps : 31\teps : 0.100\tMax.Score : 121.000\n",
      "Episode : 4700\tAverage Score : 31.24\tMax_steps : 39\teps : 0.095\tMax.Score : 121.000\n",
      "Episode : 4800\tAverage Score : 31.53\tMax_steps : 31\teps : 0.091\tMax.Score : 121.000\n",
      "Episode : 4900\tAverage Score : 30.34\tMax_steps : 35\teps : 0.086\tMax.Score : 121.000\n",
      "Episode : 5000\tAverage Score : 31.14\tMax_steps : 25\teps : 0.082\tMax.Score : 121.000\n",
      "Elapsed : 0:00:22.750476\n",
      "2019-02-08 11:09:28.951743\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Score 121.000000 at 856\n",
      "Percentile [25,50,75] : [21. 27. 33.]\n",
      "Variance : 83.717\n"
     ]
    }
   ],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "start_time = time.time()\n",
    "scores, QVal = q_learning(n_episodes=5000)\n",
    "# env.close() # Close the environment\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "print('Max Score {:2f} at {}'.format(np.max(scores), np.argmax(scores)))\n",
    "print('Percentile [25,50,75] : {}'.format(np.percentile(scores,[25,50,75])))\n",
    "print('Variance : {:.3f}'.format(np.var(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k,v in QVal.items():\n",
    "    print('k={},v={}'.format(k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the estimated optimal policy\n",
    "QPolicy={}\n",
    "for k,v in QVal.items():\n",
    "    QPolicy[k] = [0] * num_actions\n",
    "    QPolicy[k][np.argmax(QVal[k])] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points to Ponder\n",
    "1. Like our earlier lab we can decay $\\epsilon$ in different ways\n",
    "2. Even after 30,000 episodes it doesn't solve the environment.\n",
    "    * There is opportunity for tweaking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 29 steps with a Total Reward = 29\n",
      "Episode 2 finished after 25 steps with a Total Reward = 25\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(2): \n",
    "    state = env.reset()\n",
    "    tot_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        if getState(state) in QPolicy:\n",
    "            probs = QPolicy[getState(state)]\n",
    "        else:\n",
    "            print(\".\")\n",
    "            probs = [1.0/num_actions] * num_actions\n",
    "        action = np.random.choice(np.arange(num_actions), p=probs)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # print('[',state,']',' -> ', action,' = [',next_state,']', reward)\n",
    "        tot_reward += reward\n",
    "        steps += 1\n",
    "        if done:\n",
    "            print('Episode {:d} finished after {:d} steps with a Total Reward = {:.0f}'.\n",
    "                  format(i_episode+1,steps, tot_reward))\n",
    "            break\n",
    "        else:\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _That's all Folks !_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
