{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On #4 - Balancing Cart Pole w/Q-learning\n",
    "\n",
    "## Goal:\n",
    "* Introduce the CartPole Environment\n",
    "    * More complex, Continuous\n",
    "* Implement Q-Learning for digitized CartPole\n",
    "    * Later we will use function approx and remove the requirement for digitization\n",
    "\n",
    "## Steps:\n",
    "1. Get familiar with Cartpole environment\n",
    "    * np.linspace() and np.digitize() for state space aggregation\n",
    "2. Program Q Learning - _4 To Dos_\n",
    "3. Track & Plot Metrics to solve Cart Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : \n",
    "* Based on Udacity github https://github.com/udacity/deep-reinforcement-learning/tree/master/monte-carlo plus\n",
    "* My solution for the DQN https://github.com/xsankar/DQN_Navigation/blob/master/Navigation-v2.ipynb\n",
    "* Kaggle https://www.kaggle.com/sandovaledwin/q-learning-algorithm-for-solving-frozenlake-game/notebook\n",
    "* Phil Tabor's RL in Motion github https://github.com/philtabor/Reinforcement-Learning-In-Motion/tree/master/Unit-7-The-Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install the required packages\n",
    "\n",
    "* No esoteric requirements\n",
    "* You can run them without docker\n",
    "* pip install -r requirements.txt\n",
    "* Requirements\n",
    " * python 3.6, pytorch, openAI gym, numpy, matplotlib\n",
    " * anaconda is easier but not needed\n",
    " * Miniconda works fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define imports\n",
    "\n",
    "python 3, numpy, matplotlib, torch, gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import gym\n",
    "import PIL # for in-line display of certain environments\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Global Constants and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants Definitions\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "# Number of neurons in the layers of the Q Network\n",
    "FC1_UNITS = 16\n",
    "FC2_UNITS = 8\n",
    "FC3_UNITS = 4\n",
    "# Store models flag. Store during calibration runs and do not store during hyperparameter search\n",
    "STORE_MODELS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20943951023931956\n",
      "-0.20943951023931956\n"
     ]
    }
   ],
   "source": [
    "# Work area to quickly test utility functions\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "'''\n",
    "start_time = time.time()\n",
    "time.sleep(10)\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "'''\n",
    "print(math.radians(12))\n",
    "print(math.radians(-12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Create instance & Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.01866453, -0.00144507, -0.00074703,  0.00939201])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym, PIL\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "# array = env.reset()\n",
    "# ** render doesn't work reliably on a server. Uncomment when running ** locally **\n",
    "# env.render()\n",
    "# PIL.Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This what it will look like\n",
    "### We don't need the render(). We run it on headless mode and inspect the results\n",
    "<img src=\"CartPole_Render.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Examine the State and Action Spaces\n",
    "\n",
    "* The state space is continuous, with an observation space of 4 \n",
    "    * {x,$\\dot{x}$,$\\theta$, theta_dot}\n",
    "        * Cart Position,  Cart Velocity, Pole Angle, Pole Velocity at tip\n",
    "        * The angle, probably, is in radians\n",
    "\n",
    "The action space, on the contrary is simple viz. 0 = Left, 1 = Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n",
      "[0, 1]\n",
      "[ 0 = Left, 1 = Right ]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "act_space = [i for i in range(0,env.action_space.n)]\n",
    "print(act_space)\n",
    "# env.unwrapped.get_action_meanings() # AttributeError: 'FrozenLakeEnv' object has no attribute 'get_action_meanings'\n",
    "print('[ 0 = Left, 1 = Right ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_elapsed_seconds', '_elapsed_steps', '_episode_started_at', '_max_episode_seconds', '_max_episode_steps', '_past_limit', 'action_space', 'class_name', 'close', 'compute_reward', 'env', 'metadata', 'observation_space', 'render', 'reset', 'reward_range', 'seed', 'spec', 'step', 'unwrapped']\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'action_space', 'close', 'force_mag', 'gravity', 'kinematics_integrator', 'length', 'masscart', 'masspole', 'metadata', 'np_random', 'observation_space', 'polemass_length', 'render', 'reset', 'reward_range', 'seed', 'spec', 'state', 'step', 'steps_beyond_done', 'tau', 'theta_threshold_radians', 'total_mass', 'unwrapped', 'viewer', 'x_threshold']\n",
      "States =  Box(4,)\n",
      "Actions =  Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(dir(env))\n",
    "print(dir(env.unwrapped))\n",
    "print('States = ',env.unwrapped.observation_space)\n",
    "print('Actions = ',env.unwrapped.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test the environment with Random Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-On To Do - #1 to #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(3):\n",
    "    # To Do\n",
    "    # 1. Get initial State\n",
    "    # state = <fillin and uncomment>\n",
    "    tot_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        # To Do\n",
    "        # 2. Get random action - sampling env.action_space \n",
    "        # action = <fillin and uncomment>\n",
    "        # 3. take a step in the environment and get state, reward, done and info\n",
    "        # <fillin and uncomment> = <fillin and uncomment>\n",
    "        print('[',state,']','->', action,' : [',next_state,']', 'R=',reward)\n",
    "        # env.render()\n",
    "        tot_reward += reward\n",
    "        steps += 1\n",
    "        if done:\n",
    "            print('Episode {:d} finished after {:d} steps with a Total Reward = {:.0f}'.format(i_episode+1,steps, tot_reward))\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "# Pole angle +/-12 degrees, Cart Pos +/- 2.4 or 200 steps\n",
    "# Cart Pos, Velocity, Pole Angle, Velocity\n",
    "# 12 degrees = .2094 radians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning\n",
    "### Let us implement the Basic Q-Learning Algorithm\n",
    "<img src='Qlearning_Alg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Define policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-Greedy\n",
    "\n",
    "<img src=\"e_greedy.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_e_greedy_action(env,Q,state,epsilon,nA):\n",
    "    action = np.random.choice(np.arange(nA), p=get_probs(Q[state], epsilon, nA)) if state in Q else env.action_space.sample()\n",
    "    return action\n",
    "\n",
    "def get_probs(Q_s, epsilon, nA):\n",
    "    \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \"\"\"\n",
    "    policy_s = np.ones(nA) * epsilon / nA\n",
    "    best_a = np.argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "    return policy_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to convert the continuous state space to discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discretize the spaces\n",
    "poleThetaSpace = np.linspace(-0.20943951, 0.20943951, 10)\n",
    "poleThetaVelSpace = np.linspace(-4, 4, 10)\n",
    "cartPosSpace = np.linspace(-2.4, 2.4, 10)\n",
    "cartVelSpace = np.linspace(-4, 4, 10)\n",
    "\n",
    "def getState(observation):\n",
    "    cartX, cartXdot, cartTheta, cartThetadot = observation\n",
    "    cartX = int(np.digitize(cartX, cartPosSpace))\n",
    "    cartXdot = int(np.digitize(cartXdot, cartVelSpace))\n",
    "    cartTheta = int(np.digitize(cartTheta, poleThetaSpace))\n",
    "    cartThetadot = int(np.digitize(cartThetadot, poleThetaVelSpace))\n",
    "\n",
    "    return (cartX, cartXdot, cartTheta, cartThetadot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-On To Do - #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses global variavle env\n",
    "def q_learning(n_episodes=2000, max_t=1000, \n",
    "               epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.9995, # 0.995 gets to min by 1000 episodes\n",
    "               alpha=0.01, gamma=1.0):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        epsilon_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        epsilon_end (float): minimum value of epsilon\n",
    "        epsilon_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        alpha = step-size parameter\n",
    "        gamma = discount rate\n",
    "    \"\"\"\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(num_actions))\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    epsilon = epsilon_start            # initialize epsilon\n",
    "    has_seen_13 = False\n",
    "    max_score = 0\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        score = 0\n",
    "        max_steps = 0\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, n_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        s_t = env.reset()\n",
    "        a_t = choose_e_greedy_action(env,Q,getState(s_t),epsilon,num_actions)\n",
    "        t = 0\n",
    "        while True:\n",
    "            s_t_d = getState(s_t)\n",
    "            # state, reward, done, info = env.step(action)\n",
    "            s_t_1, reward, done, prob = env.step(a_t)\n",
    "            # print(state,reward,done, prob)\n",
    "            s_t_1_d = getState(s_t_1) # Digitize state\n",
    "            a_t_1 = choose_e_greedy_action(env,Q,s_t_1_d,epsilon,num_actions)\n",
    "            best_a = np.argmax(Q[s_t_1_d])\n",
    "            # *** To Do\n",
    "            # 4. Update the appripriate Q value\n",
    "            # Q[s_t_d][a_t] = <fillin and uncomment>\n",
    "            a_t = a_t_1\n",
    "            s_t = s_t_1\n",
    "            score += reward\n",
    "            max_steps += 1\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        epsilon = max(epsilon*epsilon_decay, epsilon_min) # decrease epsilon\n",
    "        print('\\rEpisode : {}\\tAverage Score : {:5.2f}\\tMax_steps : {}\\teps : {:5.3f}\\tMax.Score : {:5.3f}'.\\\n",
    "              format(i_episode, np.mean(scores_window),max_steps,epsilon,max_score), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode : {}\\tAverage Score : {:5.2f}\\tMax_steps : {}\\teps : {:5.3f}\\tMax.Score : {:5.3f}'.\\\n",
    "                  format(i_episode, np.mean(scores_window),max_steps,epsilon,max_score))\n",
    "        if (np.mean(scores_window)>=195.0) and (not has_seen_13):\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:5.2f}'.\\\n",
    "                  format(i_episode-100, np.mean(scores_window)))\n",
    "            # torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            has_seen_13 = True\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "    return scores, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 100\tAverage Score : 21.62\tMax_steps : 15\teps : 0.951\tMax.Score : 61.000\n",
      "Episode : 200\tAverage Score : 23.32\tMax_steps : 8\teps : 0.905\tMax.Score : 66.0000\n",
      "Episode : 300\tAverage Score : 22.94\tMax_steps : 36\teps : 0.861\tMax.Score : 91.000\n",
      "Episode : 400\tAverage Score : 21.04\tMax_steps : 25\teps : 0.819\tMax.Score : 91.000\n",
      "Episode : 500\tAverage Score : 20.69\tMax_steps : 23\teps : 0.779\tMax.Score : 91.000\n",
      "Episode : 600\tAverage Score : 19.66\tMax_steps : 26\teps : 0.741\tMax.Score : 91.000\n",
      "Episode : 700\tAverage Score : 18.60\tMax_steps : 23\teps : 0.705\tMax.Score : 91.000\n",
      "Episode : 800\tAverage Score : 16.70\tMax_steps : 25\teps : 0.670\tMax.Score : 91.000\n",
      "Episode : 900\tAverage Score : 16.58\tMax_steps : 27\teps : 0.638\tMax.Score : 91.000\n",
      "Episode : 1000\tAverage Score : 14.83\tMax_steps : 13\teps : 0.606\tMax.Score : 91.000\n",
      "Episode : 1100\tAverage Score : 15.86\tMax_steps : 10\teps : 0.577\tMax.Score : 91.000\n",
      "Episode : 1200\tAverage Score : 14.74\tMax_steps : 12\teps : 0.549\tMax.Score : 91.000\n",
      "Episode : 1300\tAverage Score : 14.22\tMax_steps : 10\teps : 0.522\tMax.Score : 91.000\n",
      "Episode : 1400\tAverage Score : 13.67\tMax_steps : 12\teps : 0.496\tMax.Score : 91.000\n",
      "Episode : 1500\tAverage Score : 13.96\tMax_steps : 18\teps : 0.472\tMax.Score : 91.000\n",
      "Episode : 1600\tAverage Score : 13.00\tMax_steps : 11\teps : 0.449\tMax.Score : 91.000\n",
      "Episode : 1700\tAverage Score : 13.28\tMax_steps : 9\teps : 0.427\tMax.Score : 91.0000\n",
      "Episode : 1800\tAverage Score : 12.37\tMax_steps : 10\teps : 0.406\tMax.Score : 91.000\n",
      "Episode : 1900\tAverage Score : 12.35\tMax_steps : 12\teps : 0.387\tMax.Score : 91.000\n",
      "Episode : 2000\tAverage Score : 12.31\tMax_steps : 13\teps : 0.368\tMax.Score : 91.000\n",
      "Episode : 2100\tAverage Score : 12.28\tMax_steps : 15\teps : 0.350\tMax.Score : 91.000\n",
      "Episode : 2200\tAverage Score : 11.44\tMax_steps : 8\teps : 0.333\tMax.Score : 91.0000\n",
      "Episode : 2300\tAverage Score : 11.59\tMax_steps : 12\teps : 0.317\tMax.Score : 91.000\n",
      "Episode : 2400\tAverage Score : 11.60\tMax_steps : 10\teps : 0.301\tMax.Score : 91.000\n",
      "Episode : 2500\tAverage Score : 11.23\tMax_steps : 12\teps : 0.286\tMax.Score : 91.000\n",
      "Episode : 2600\tAverage Score : 11.05\tMax_steps : 10\teps : 0.272\tMax.Score : 91.000\n",
      "Episode : 2700\tAverage Score : 10.54\tMax_steps : 10\teps : 0.259\tMax.Score : 91.000\n",
      "Episode : 2800\tAverage Score : 11.02\tMax_steps : 11\teps : 0.247\tMax.Score : 91.000\n",
      "Episode : 2900\tAverage Score : 10.89\tMax_steps : 10\teps : 0.234\tMax.Score : 91.000\n",
      "Episode : 3000\tAverage Score : 10.60\tMax_steps : 10\teps : 0.223\tMax.Score : 91.000\n",
      "Episode : 3100\tAverage Score : 10.52\tMax_steps : 10\teps : 0.212\tMax.Score : 91.000\n",
      "Episode : 3200\tAverage Score : 10.54\tMax_steps : 10\teps : 0.202\tMax.Score : 91.000\n",
      "Episode : 3300\tAverage Score : 10.55\tMax_steps : 9\teps : 0.192\tMax.Score : 91.0000\n",
      "Episode : 3400\tAverage Score : 10.52\tMax_steps : 8\teps : 0.183\tMax.Score : 91.0000\n",
      "Episode : 3500\tAverage Score : 10.58\tMax_steps : 10\teps : 0.174\tMax.Score : 91.000\n",
      "Episode : 3600\tAverage Score : 10.41\tMax_steps : 11\teps : 0.165\tMax.Score : 91.000\n",
      "Episode : 3700\tAverage Score : 10.39\tMax_steps : 10\teps : 0.157\tMax.Score : 91.000\n",
      "Episode : 3800\tAverage Score : 10.05\tMax_steps : 12\teps : 0.149\tMax.Score : 91.000\n",
      "Episode : 3900\tAverage Score : 10.10\tMax_steps : 12\teps : 0.142\tMax.Score : 91.000\n",
      "Episode : 4000\tAverage Score : 10.28\tMax_steps : 12\teps : 0.135\tMax.Score : 91.000\n",
      "Episode : 4100\tAverage Score : 10.31\tMax_steps : 10\teps : 0.129\tMax.Score : 91.000\n",
      "Episode : 4200\tAverage Score : 10.02\tMax_steps : 9\teps : 0.122\tMax.Score : 91.0000\n",
      "Episode : 4300\tAverage Score : 10.07\tMax_steps : 9\teps : 0.116\tMax.Score : 91.0000\n",
      "Episode : 4400\tAverage Score : 10.35\tMax_steps : 11\teps : 0.111\tMax.Score : 91.000\n",
      "Episode : 4500\tAverage Score :  9.85\tMax_steps : 10\teps : 0.105\tMax.Score : 91.000\n",
      "Episode : 4600\tAverage Score :  9.90\tMax_steps : 11\teps : 0.100\tMax.Score : 91.000\n",
      "Episode : 4700\tAverage Score : 10.03\tMax_steps : 9\teps : 0.095\tMax.Score : 91.0000\n",
      "Episode : 4800\tAverage Score :  9.69\tMax_steps : 9\teps : 0.091\tMax.Score : 91.0000\n",
      "Episode : 4900\tAverage Score :  9.85\tMax_steps : 9\teps : 0.086\tMax.Score : 91.0000\n",
      "Episode : 5000\tAverage Score :  9.85\tMax_steps : 8\teps : 0.082\tMax.Score : 91.0000\n",
      "Elapsed : 0:00:12.215245\n",
      "2019-02-08 10:57:20.020035\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Score 91.000000 at 284\n",
      "Percentile [25,50,75] : [10. 11. 14.]\n",
      "Variance : 40.459\n"
     ]
    }
   ],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "start_time = time.time()\n",
    "scores, QVal = q_learning(n_episodes=5000)\n",
    "# env.close() # Close the environment\n",
    "print('Elapsed : {}'.format(timedelta(seconds=time.time() - start_time)))\n",
    "print(datetime.now())\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "print('Max Score {:2f} at {}'.format(np.max(scores), np.argmax(scores)))\n",
    "print('Percentile [25,50,75] : {}'.format(np.percentile(scores,[25,50,75])))\n",
    "print('Variance : {:.3f}'.format(np.var(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k,v in QVal.items():\n",
    "    print('k={},v={}'.format(k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the estimated optimal policy\n",
    "QPolicy={}\n",
    "for k,v in QVal.items():\n",
    "    QPolicy[k] = [0] * num_actions\n",
    "    QPolicy[k][np.argmax(QVal[k])] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points to Ponder\n",
    "1. Like our earlier lab we can decay $\\epsilon$ in different ways\n",
    "2. Even after 30,000 episodes it doesn't solve the environment.\n",
    "    * There is opportunity for tweaking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 10 steps with a Total Reward = 10\n",
      "Episode 2 finished after 9 steps with a Total Reward = 9\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(2): \n",
    "    state = env.reset()\n",
    "    tot_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        if getState(state) in QPolicy:\n",
    "            probs = QPolicy[getState(state)]\n",
    "        else:\n",
    "            print(\".\")\n",
    "            probs = [1.0/num_actions] * num_actions\n",
    "        action = np.random.choice(np.arange(num_actions), p=probs)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # print('[',state,']',' -> ', action,' = [',next_state,']', reward)\n",
    "        tot_reward += reward\n",
    "        steps += 1\n",
    "        if done:\n",
    "            print('Episode {:d} finished after {:d} steps with a Total Reward = {:.0f}'.\n",
    "                  format(i_episode+1,steps, tot_reward))\n",
    "            break\n",
    "        else:\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _That's all Folks !_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
